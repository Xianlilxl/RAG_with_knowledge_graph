"""
@Authors:
* Xianli Li (xli@assystem.com)
This script is designed to generate responses based on an annotated dataset. 
The responses generated by a Language Model (LLM) are then compared with the ground truth. 
The incorrect predictions, along with their corresponding ground truth labels, 
are utilized in Direct Preference Optimization (DPO) training to enhance the model's performance.
"""
import os
import wandb
import torch
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import Dict, List, Generator
from datasets import load_dataset, load_from_disk
from peft import PeftModel
from transformers import GenerationConfig
import json 
import ast

BASE_MODEL = "mistralai/Mistral-7B-v0.1"


class Model:
    def __init__(self, checkpoint_dir: str) -> None:
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print("THE DEVICE INFERENCE IS RUNNING ON IS: ", self.device)
        self.tokenizer = None
        self.pipeline = None
        self.stopping_criteria = None
        self.checkpoint_dir = checkpoint_dir
    
    def get_checkpoint_dir(self):
        run = wandb.init()
        checkpoint = run.use_artifact(self.wandb_checkpoint_name, type='model')
        checkpoint_dir = checkpoint.download()
        return checkpoint_dir

    def load(self):
        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,  # Mistral, same as before
            # quantization_config=QUANTIZATION_CONFIG,  # Same quantization config as before
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )

        # load and merge the checkpoint with base model
        self.ft_model = PeftModel.from_pretrained(self.base_model, self.checkpoint_dir)
        self.ft_model.eval()

        self.gen_cfg = GenerationConfig.from_model_config(self.ft_model.config)
        self.gen_cfg.max_new_tokens = 1024
        self.gen_cfg.temperature = 0.5
        self.gen_cfg.num_return_sequences = 1
        self.gen_cfg.use_cache = True
        self.gen_cfg.min_length = 1

    def predict(self, request: Dict) -> Dict | Generator:
        with torch.no_grad():
            prompt = request.pop("prompt")
            inputs = self.tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"].cuda()
            generation_output = self.ft_model.generate(
                input_ids=input_ids,
                pad_token_id=self.tokenizer.eos_token_id,
                generation_config=self.gen_cfg,
                return_dict_in_generate=True,
                output_scores=True,
                max_new_tokens=256
            )
            outputs = []
            for seq in generation_output.sequences:
                output = self.tokenizer.decode(seq)
                outputs.append(output)

            return "\n".join(outputs)

artifact_dir = "./artifacts/checkpoint-r641yots:v10"
ft_model = Model(checkpoint_dir=artifact_dir)
ft_model.load()

def get_prompt_grand_truth(test_example):
    try:
        prompt, grand_truth = tuple(test_example["text"].split("### ENTITES:"))
    except:
        prompt, grand_truth = tuple(test_example["text"].split("### ENTITIES:"))
    prompt+="### RELATIONS:\n"
    test_example["prompt"] = prompt
    grand_truth = grand_truth.split("### RELATIONS:\n")[1]
    grand_truth = grand_truth.replace("</s>", "").replace("\n", "")
    grand_truth_dict = ast.literal_eval(grand_truth)
    test_example["grand_truth"] = grand_truth_dict

    return test_example

def get_prediction(test_example):
    output = ft_model.predict(request={
        "prompt": test_example["prompt"],
        "temperature": 0.5, 
        "max_new_tokens": 1024
    })
    # output_dict = string_to_dict(output)
    test_example["prediction"] = output
    return test_example

test_dataset = load_from_disk("./datasets/visionary-spaceship-64/SREDFM-dataset:v2/test")
GT_test_dataset = test_dataset.map(get_prompt_grand_truth)
GT_pred_dataset = GT_test_dataset.map(get_prediction)
GT_pred_dataset.save_to_disk("./datasets/GT_pred_dataset")