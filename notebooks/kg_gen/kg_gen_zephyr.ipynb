{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG generation with Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from transformers import BitsAndBytesConfig\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from typing import Optional, List, Mapping, Any, Tuple\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index import (\n",
    "    ServiceContext, \n",
    "    SimpleDirectoryReader, \n",
    "#     LangchainEmbedding, \n",
    "#     ListIndex,\n",
    "    KnowledgeGraphIndex\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager\n",
    "from llama_index.llms import (\n",
    "    CustomLLM, \n",
    "    CompletionResponse, \n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "from llama_index.llms.base import llm_completion_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Customize LLM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZephyrEndpointLLM(CustomLLM):\n",
    "    api_endpoint: str\n",
    "    endpoint_path: str = \"/v1/models/model:predict\"\n",
    "\n",
    "    context_window: int = 2048\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(\n",
    "        self, prompt: str, \n",
    "        stop: Optional[List[str]] = [],\n",
    "        temperature: float = 0.5,\n",
    "        max_new_tokens: int = 1024,\n",
    "        **kwargs: Any) -> CompletionResponse:\n",
    "        # prompt_length = len(prompt)\n",
    "        # response = pipeline(prompt, max_new_tokens=self.num_output)[0][\"generated_text\"]\n",
    "\n",
    "        # # only return newly generated tokens\n",
    "        # text = response[prompt_length:]\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop\": stop or [],\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.api_endpoint + self.endpoint_path, json=data)\n",
    "            if response.status_code == 200:\n",
    "                text = dict(response.json())['data']['generated_text']\n",
    "            else:\n",
    "                raise ValueError(f'The response status code was: {response.status_code}, '\n",
    "                                 'expected: 200')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise SystemExit(e)\n",
    "\n",
    "        return CompletionResponse(text=text)\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n",
    "    \"Some text is provided below. Given the text, extract up to \"\n",
    "    \"{max_knowledge_triplets} \"\n",
    "    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Example:\"\n",
    "    \"Text: Alice is Bob's mother.\"\n",
    "    \"Triplets:\\n(Alice, is mother of, Bob)\\n\"\n",
    "    \"Text: Philz is a coffee shop founded in Berkeley in 1982.\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(Philz, is, coffee shop)\\n\"\n",
    "    \"(Philz, founded in, Berkeley)\\n\"\n",
    "    \"(Philz, founded in, 1982)\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    # \"Text: {text}\\n\"\n",
    "    # \"Triplets:\\n\"\n",
    ")\n",
    "user_prompt = (\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Triplets:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Some text is provided below. Given the text, extract up to {max_knowledge_triplets} knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\n",
      "---------------------\n",
      "Example:Text: Alice is Bob's mother.Triplets:\n",
      "(Alice, is mother of, Bob)\n",
      "Text: Philz is a coffee shop founded in Berkeley in 1982.\n",
      "Triplets:\n",
      "(Philz, is, coffee shop)\n",
      "(Philz, founded in, Berkeley)\n",
      "(Philz, founded in, 1982)\n",
      "---------------------\n",
      "</s>\n",
      "<|user|>\n",
      "Text: {text}\n",
      "Triplets:\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_KG_TRIPLET_EXTRACT_TMPL,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompt_type import PromptType\n",
    "KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n",
    "    prompt, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/xli/.cache/torch/sentence_transformers/dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# define our LLM\n",
    "llm = ZephyrEndpointLLM(api_endpoint=\"http://127.0.0.1:8080\")\n",
    "\n",
    "embed_model = HuggingFaceBgeEmbeddings(model_name=\"dangvantuan/sentence-camembert-large\")\n",
    "\n",
    "context_window = 2048\n",
    "# set number of output tokens\n",
    "num_output = 1024\n",
    "chunk_size = 512\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model,\n",
    "    context_window=context_window, \n",
    "    chunk_size=chunk_size,\n",
    "    num_output=num_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect with nebula graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from nebula3.gclient.net import Connection\n",
    "from nebula3.gclient.net.SessionPool import SessionPool\n",
    "from nebula3.Config import SessionPoolConfig\n",
    "from nebula3.common.ttypes import ErrorCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\"\n",
    "os.environ[\"GRAPHD_HOST\"] = \"127.0.0.1\"\n",
    "os.environ[\"GRAPHD_PORT\"] = \"9669\"\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\"\n",
    "space_name = \"Digital_Safety\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SessionPoolConfig()\n",
    "\n",
    "# prepare space\n",
    "conn = Connection()\n",
    "conn.open(os.environ[\"GRAPHD_HOST\"], os.environ[\"GRAPHD_PORT\"], 1000)\n",
    "auth_result = conn.authenticate(os.environ[\"NEBULA_USER\"], os.environ[\"NEBULA_PASSWORD\"])\n",
    "assert auth_result.get_session_id() != 0\n",
    "resp = conn.execute(\n",
    "    auth_result._session_id,\n",
    "    \"CREATE SPACE IF NOT EXISTS \"+space_name+\"(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\",\n",
    ")\n",
    "assert resp.error_code == ErrorCode.SUCCEEDED\n",
    "# insert data need to sleep after create schema\n",
    "time.sleep(10)\n",
    "\n",
    "session_pool = SessionPool(os.environ[\"NEBULA_USER\"], os.environ[\"NEBULA_PASSWORD\"], space_name, [(os.environ[\"GRAPHD_HOST\"], os.environ[\"GRAPHD_PORT\"])])\n",
    "assert session_pool.init(config)\n",
    "\n",
    "# add schema\n",
    "resp = session_pool.execute(\n",
    "    'CREATE TAG IF NOT EXISTS entity(name string);'\n",
    "    'CREATE EDGE IF NOT EXISTS relationship(relationship string);'\n",
    "    'CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = os.environ[\"NEBULA_USER\"]\n",
    "os.environ['NEBULA_PASSWORD'] = os.environ[\"NEBULA_PASSWORD\"]\n",
    "os.environ['NEBULA_ADDRESS'] = os.environ[\"NEBULA_ADDRESS\"]\n",
    "\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the your data\n",
    "documents = SimpleDirectoryReader(\"../../../data/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='9967d40e-fc12-4227-b1c4-57ff9fda1af7', embedding=None, metadata={'page_label': '1', 'file_name': 'Digital Safety_Livrable1_Etat de l_art_RCO-5.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f19a671fc007c757599848b2bbf57dfc0acde5e68f204fe4125c3c09509371e5', text=\" NOTE TECHNIQUE   Etat de l’art sur la cartographie automatique et dynamique des relations   intras et inters documentaires des documents composant un rapport de sûreté pour le Projet I1 « Digital Safety » \\n5  \\nNOTE TECHNIQUE – CARTOGRAPHIE AUTOMATIQUE ET DYNAMIQUE DES RELATIONS INTERNES ET ENTRE LES DOCUMENTS Contexte Dans le cadre du projet I1 « Digital Safety », ASSYSTEM souhaite développer une solution qui s’appuierait sur une Intelligence Artificielle (IA) pour cartographier de façon automatique et dynamique les relations intras et inters documentaires des documents composant un rapport de sûreté. Cette note technique présente un état de l’art sur les algorithmes de création de cartographies documentaires par IA. Problématique Un rapport de sûreté nucléaire est un document complexe, volumineux et très détaillé qui décrit tous les aspects liés à la sûreté d'une installation nucléaire. Il est conçu pour fournir aux régulateurs (i.e. l'Autorité de Sûreté Nucléaire ASN en France) une information complète et claire sur la façon dont la sûreté nucléaire est gérée et assurée. Le rapport comprend une multitude de documents. En termes d'ordre de grandeur, il pourrait facilement être composé de milliers, voire de dizaines de milliers de pages, en fonction de la taille, de la complexité et du type d'installation nucléaire concernée. La gestion des modifications dans un rapport de sûreté nucléaire peut être une tâche complexe et rigoureuse en raison du volume de documents interdépendants. Quand une modification est apportée à un document, elle peut avoir des impacts en cascade sur plusieurs autres documents. Étant donné le volume massif de documents constituant le rapport de sûreté, il est indispensable de s'appuyer sur des systèmes et des logiciels automatisés pour gérer efficacement les modifications et évaluer leurs impacts potentiels sur les autres documents ou sections du rapport. Il est cependant à souligner que les documents qui composent ce rapport sont « non structurés » dans des bases de données, ce sont des documents écrits en langage naturel.  L’IA objet de cette note est destinée à cartographier automatiquement et de manière dynamique les relations intras et inters documentaires des documents composant un rapport de sûreté. Lorsqu'une modification est introduite dans l'un des documents, cette IA serait capable d'alerter l'ingénieur en sûreté nucléaire sur les autres documents susceptibles d'être impactés par cette modification. Cette note a donc pour objectif de présenter un état de l'art sur la cartographie automatique et dynamique des relations internes et entre les documents, jetant ainsi les bases pour le développement futur de cette IA.   \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36f469a4-8e38-412d-ba7c-5ca67b8274a8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3809b4cc308135d3cb401471e72020f6b2e6134b9e8494cbac4e1aa0ef389962', text='[ZoneTransfer]\\nZoneId=3\\nReferrerUrl=https://www.ilovepdf.com/\\nHostUrl=https://api50.ilovepdf.com/v1/download/p0d2xAp96wvp5jhtwvfl5k02lbly3f5yn78q4n7731gwcz2pf4r9r73mydnnfyn8s8zjjjbt6rdlpm41920w312xwgvsc1pk3vs30n784Aqd13AqyA1zw9htnf7vvdf289tvwcm0v2wj0vwmm8t453kAp8y2tA3xm1dh6y1wt95tknsf18z1\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Project I1 \"Digital Safety\" - seeks - AI solution for automated and dynamic mapping of intra and interdocument relations in safety reports\n",
      "2. Safety report - is - a complex, voluminous, and detailed document describing all aspects related to nuclear safety for an installation\n",
      "3. Safety report - consists of - a multitude of documents, ranging from thousands to tens of thousands of pages, depending on the size, complexity, and type of the nuclear installation\n",
      "4. Project I1 \"Digital Safety\" - aims to develop - an AI-based solution for automated and dynamic mapping of intra and interdocument relations in safety reports\n",
      "5. AI-based solution - can - automate and dynamically map intra and interdocument relations in safety reports\n",
      "6. Safety report - includes - a multitude of interdependent documents\n",
      "7. Project I1 \"Digital Safety\" - requires - an AI-based solution for automated and dynamic mapping of intra and interdocument relations in safety reports due to the complexity and rigor involved in managing modifications in safety reports\n",
      "8. AI-based solution - provides - automated and dynamic mapping of intra and interdocument relations in safety reports\n",
      "9. Project I1 \"Digital Safety\" - utilizes - AI technology for automated and dynamic mapping of intra and interdocument relations in safety reports\n",
      "10. Safety report - involves - a multitude of interdependent documents, making the management of modifications a complex and rigorous task.\n",
      "1. Page_label: 1\n",
      "file_name: Digital Safety_Livrable1_Etat de l_art_RCO-5.pdf (subject)\n",
      "is associated with (predicate)\n",
      "automatic and dynamic document relationship mapping (object)\n",
      "\n",
      "2. Page_label: 1\n",
      "file_name: Digital Safety_Livrable1_Etat de l_art_RCO-5.pdf (subject)\n",
      "is related to (predicate)\n",
      "IA for automatically and dynamically mapping document relationships (object)\n",
      "\n",
      "3. IA for automatically and dynamically mapping document relationships (subject)\n",
      "is intended to (predicate)\n",
      "cartographically map intras and inters document relationships in a nuclear safety report (object)\n",
      "\n",
      "4. IA for automatically and dynamically mapping document relationships (subject)\n",
      "is capable of (predicate)\n",
      "alerting a nuclear safety engineer about other documents that may be impacted by a modification (object)\n",
      "\n",
      "5. Nuclear safety report (subject)\n",
      "consists of (predicate)\n",
      "non-structured documents in natural language (object)\n",
      "\n",
      "6. Non-structured documents in natural language (subject)\n",
      "are managed (predicate)\n",
      "efficiently by automated systems and software for handling modification impacts (object)\n",
      "\n",
      "7. Modification (subject)\n",
      "can have (predicate)\n",
      "cascading impacts on multiple other documents (object)\n",
      "\n",
      "8. Document (subject)\n",
      "can be impacted (predicate)\n",
      "by a modification to another document (object)\n",
      "\n",
      "9. Automated systems and software for handling modification impacts (subject)\n",
      "are essential (predicate)\n",
      "for managing the vast volume of documents in a nuclear safety report (object)\n",
      "\n",
      "10. Cartography (subject)\n",
      "is the process of (predicate)\n",
      "automatically and dynamically mapping intras and inters document relationships in a nuclear safety report (object)\n",
      "(ZoneTransfer, has zoneId, 3)\n",
      "(ReferrerUrl, has value, https://www.ilovepdf.com/)\n",
      "(HostUrl, has value, https://api50.ilovepdf.com/)\n",
      "(ReferrerUrl, is, https://www.ilovepdf.com/)\n",
      "(HostUrl, is, https://api50.ilovepdf.com/)\n",
      "\n",
      "Note: These triplets are not directly related to the given text, as the text only provides the ZoneTransfer, ReferrerUrl, and HostUrl values. To extract more triplets, additional context would be needed.\n"
     ]
    }
   ],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"./example.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27b73d79d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "g = kg_index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show('./example.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
