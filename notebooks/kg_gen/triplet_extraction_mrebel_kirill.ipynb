{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BartTokenizerFast, MBart50TokenizerFast\n",
    "import ray\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"true\"\n",
    "\n",
    "###################### READ FILES ######################\n",
    "def process_pages(reader):\n",
    "    complete_text = \"\"\n",
    "    for page in tqdm(range(len(reader.pages))):\n",
    "        current_page = reader.pages[page]\n",
    "        current_text = current_page.extract_text()\n",
    "        #remove hyphens\n",
    "        current_text = current_text.replace('-\\n', '')\n",
    "        #remove newlines\n",
    "        current_text = current_text.replace('\\n', ' ')\n",
    "        \n",
    "        current_text = current_text + ' '\n",
    "        complete_text = complete_text + current_text\n",
    "\n",
    "    return complete_text\n",
    "\n",
    "###################### CHUNKING ######################\n",
    "# NOTE: chunk size is set by model_max_length * MULTIPLIER\n",
    "MULTIPLIER = 0.25 \n",
    "\n",
    "def split_chunks(tokenizer, text) -> list:\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks based on a specified chunk size,\n",
    "    ensuring that chunks do not split over words.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split into chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer_type ='sentence_piece'\n",
    "    if isinstance(tokenizer, BartTokenizerFast):\n",
    "        separator = 'Ġ'\n",
    "    else:\n",
    "        separator = '▁'\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    text_chunks = list()\n",
    "    processed_tokens = list()\n",
    "    chunk_start = 0\n",
    "    while len(processed_tokens) != len(tokens):\n",
    "        # consider chunk of tokens\n",
    "        # adjust chunk size to avoid splitting over words\n",
    "        if len(tokens) - len(processed_tokens) > int(tokenizer.model_max_length * MULTIPLIER):\n",
    "            chunk_end = chunk_start + int(tokenizer.model_max_length * MULTIPLIER)\n",
    "            chunk_end = adjust_chunk_end(tokenizer_type, separator, tokens, chunk_end)\n",
    "        else:\n",
    "            chunk_end = len(tokens)\n",
    "        # select slice with chunk size\n",
    "        current_chunk_tokens = tokens[chunk_start : chunk_end]\n",
    "        current_chunk_text = tokenizer.convert_tokens_to_string(current_chunk_tokens)\n",
    "        if not check_chunk(tokenizer, current_chunk_text):\n",
    "            # remove 1 more word\n",
    "            chunk_end -= 1\n",
    "            chunk_end = adjust_chunk_end(tokenizer_type, separator, tokens, chunk_end)\n",
    "            current_chunk_tokens = tokens[chunk_start : chunk_end]\n",
    "            current_chunk_text = tokenizer.convert_tokens_to_string(current_chunk_tokens)\n",
    "        # append text slice\n",
    "        text_chunks.append(current_chunk_text) \n",
    "        processed_tokens.extend(current_chunk_tokens)\n",
    "        chunk_start = chunk_end\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "def adjust_chunk_end(tokenizer_type:str, separator:str, tokens:list, chunk_end:int) -> int:\n",
    "    # adjust chunk size if the split is on a word\n",
    "    if tokenizer_type == 'word_piece':\n",
    "        separator='##'\n",
    "                # check if last 2 symbols are not the separator\n",
    "        while tokens[chunk_end][:2] == separator:\n",
    "            chunk_end -= 1\n",
    "    elif tokenizer_type == 'sentence_piece':\n",
    "        # separator='▁'\n",
    "                # check if first symbol is not separator\n",
    "        while tokens[chunk_end][0] != separator:\n",
    "            chunk_end -= 1\n",
    "    else:\n",
    "        raise Exception('Invalid tokenizer type')\n",
    "    \n",
    "    return chunk_end\n",
    "\n",
    "def check_chunk(tokenizer, chunk:str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether a given text chunk is within the allowed chunk size.\n",
    "\n",
    "    Args:\n",
    "        chunk (str): The text chunk to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the chunk is within the allowed size, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(chunk)\n",
    "    if len(tokens) > int(tokenizer.model_max_length * MULTIPLIER):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "###################### INFERENCE ######################\n",
    "# @ray.remote(num_gpus=0.5)\n",
    "class MRebelExtractor:\n",
    "    def __init__(self):\n",
    "        self.model = pipeline('translation_xx_to_yy',\n",
    "                               model='Babelscape/mrebel-large',\n",
    "                               tokenizer='Babelscape/mrebel-large',\n",
    "                               max_length=1024,\n",
    "                               device=\"cuda:0\"\n",
    "                            #    device=\"cpu\"\n",
    "                               )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, str]) -> Dict[str, list]:\n",
    "        extracted_texts = []\n",
    "        for text in batch['item']:\n",
    "            seqs = self.model(\n",
    "                text,\n",
    "                decoder_start_token_id=250058, \n",
    "                src_lang=\"fr_XX\", \n",
    "                tgt_lang=\"<triplet>\", \n",
    "                return_tensors=True, \n",
    "                return_text=False)\n",
    "\n",
    "            token_ids = seqs[0][\"translation_token_ids\"]\n",
    "            extracted_text = self.model.tokenizer.batch_decode([token_ids])\n",
    "            extracted_texts.append(extracted_text)\n",
    "\n",
    "        return {'output': extracted_texts}  \n",
    "    \n",
    "###################### EXTRACT TRIPLES ######################\n",
    "def extract_triplets_typed(text):\n",
    "    triplets = []\n",
    "    relation = ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    subject, relation, object_, object_type, subject_type = '','','','',''\n",
    "\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"tp_XX\", \"\").replace(\"__en__\", \"\").split():\n",
    "        if token == \"<triplet>\" or token == \"<relation>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token.startswith(\"<\") and token.endswith(\">\"):\n",
    "            if current == 't' or current == 'o':\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "                object_ = ''\n",
    "                subject_type = token[1:-1]\n",
    "            else:\n",
    "                current = 'o'\n",
    "                object_type = token[1:-1]\n",
    "                relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '' and object_type != '' and subject_type != '':\n",
    "        triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "    return triplets\n",
    "\n",
    "###################### CREATE KNOWLEDGE BASE ######################\n",
    "import wikipedia\n",
    "import asyncio\n",
    "\n",
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.entities = set()\n",
    "        self.ent_type_map = {}\n",
    "        self.ent_types = set()\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr].lower() == r2[attr].lower() for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r1, r)][0]\n",
    "\n",
    "    def add_entity(self, e):\n",
    "        self.entities.add(e[\"title\"])\n",
    "        self.ent_types.add(e[\"type\"])\n",
    "        self.ent_type_map.update({e[\"title\"]: e[\"type\"]})\n",
    "\n",
    "    async def get_wikipedia_data(self, candidate_entity):\n",
    "        try:\n",
    "            page = await asyncio.get_running_loop().run_in_executor(None, wikipedia.page, candidate_entity, False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary\n",
    "            }\n",
    "            return entity_data\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def bad_entity_check(self, entity):\n",
    "\n",
    "        digit_ratio = sum(c.isdigit() for c in entity) / len(entity)\n",
    "        if digit_ratio > 0.25:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def capital_count(self, string):\n",
    "        return sum(c.isupper() for c in string)\n",
    "\n",
    "    # async def add_relation(self, r):\n",
    "    def add_relation(self, r):\n",
    "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
    "        candidate_entity_types = [r[\"head_type\"], r[\"tail_type\"]]\n",
    "        #filter self-reference\n",
    "        if candidate_entities[0].lower() == candidate_entities[1].lower():\n",
    "            return\n",
    "        #filter 1 or 2 letter entities\n",
    "        if any(len(ent) < 3 for ent in candidate_entities):\n",
    "            return\n",
    "        if any(self.bad_entity_check(ent) for ent in candidate_entities):\n",
    "            return\n",
    "        \n",
    "        entities = candidate_entities # offline\n",
    "\n",
    "        # manage new entities\n",
    "        for e, ent_type in zip(entities, candidate_entity_types):\n",
    "            ent = {\"title\": e,\n",
    "                   \"type\": ent_type,\n",
    "                \"url\": '',\n",
    "                \"summary\": ''}\n",
    "            self.add_entity(ent)\n",
    "\n",
    "        # rename relation entities with their wikipedia titles\n",
    "        r[\"head\"] = entities[0]\n",
    "        r[\"tail\"] = entities[1]\n",
    "\n",
    "        # manage new relation\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for e in self.entities:\n",
    "            print(f\"  {e}\")\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "###################### CREATE KNOWLEDGE GRAPH ######################\n",
    "def save_network_html(kb, filename=\"network.html\"):\n",
    "    # create network\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    G.add_nodes_from(kb.entities)\n",
    "    G.add_edges_from((r[\"head\"], r[\"tail\"], {'relation': r['type']}) for r in kb.relations)\n",
    "\n",
    "    net = Network(directed=True, width=\"1920px\", height=\"1080px\", bgcolor=\"#eeeeee\")\n",
    " \n",
    "    # palette = itertools.cycle(sns.color_palette())\n",
    "    colors = {}\n",
    "    for ent_type, color_unit in zip(kb.ent_types, sns.color_palette().as_hex()):\n",
    "        colors[ent_type] = color_unit\n",
    "    # nodes\n",
    "    color_entity = \"#00FF00\"\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, \n",
    "                     shape=\"circle\", \n",
    "                     color=colors[kb.ent_type_map[e]]\n",
    "                    #  color=color_entity\n",
    "                     )\n",
    "\n",
    "    # edges\n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r[\"head\"], r[\"tail\"],\n",
    "                    title=r[\"type\"], label=r[\"type\"])\n",
    "        \n",
    "    # save network\n",
    "    net.repulsion(\n",
    "        node_distance=250,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=250,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename, notebook=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dir = './irsn/corpus/'\n",
    "dir_list = os.listdir(f_dir)\n",
    "dir_list = [name for name in dir_list if 'pdf' in name]\n",
    "# len(dir_list)\n",
    "corpus_dict = {\n",
    "    'text': [],\n",
    "    'file_name': []\n",
    "}\n",
    "for name in dir_list:\n",
    "    print(f'Processing {name}')\n",
    "    corpus_dict['file_name'].append(name)\n",
    "    reader = PdfReader(f_dir + name)\n",
    "    text = process_pages(reader)\n",
    "    corpus_dict['text'].append(text)\n",
    "\n",
    "corpus_df = pd.DataFrame(corpus_dict)\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously pickled results\n",
    "# with open('./irsn_corpus_df.pkl', 'rb') as handle:\n",
    "# corpus_df = pd.read_pickle(handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12774 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 58/58 [00:18<00:00,  3.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained('Babelscape/mrebel-large')\n",
    "\n",
    "chunk_dict = {\n",
    "\n",
    "}\n",
    "chunks = []\n",
    "chunk_length = 0\n",
    "for file_name, text in tqdm(zip(corpus_df.file_name.values, corpus_df.text.values), \n",
    "                                           total = len(corpus_df.text.values)):\n",
    "    text_chunks = split_chunks(tokenizer, text)\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        chunk_dict[chunk_length + i] = file_name\n",
    "    chunk_length += len(text_chunks) \n",
    "    chunks.extend(text_chunks)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 10:19:33,217\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "2023-11-21 10:19:34,173\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "2023-11-21 10:19:34,174\tINFO dataset.py:2087 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-11-21 10:19:34,177\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(MRebelExtractor)]\n",
      "2023-11-21 10:19:34,178\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=True)\n",
      "2023-11-21 10:19:34,194\tINFO actor_pool_map_operator.py:114 -- MapBatches(MRebelExtractor): Waiting for 2 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25d4fe985db42ce9cdd09af22a7a9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(MRebelExtractor) 1:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cd7025c6fe4cfa8f23c4566c7fe809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 10:20:17,016\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-7, started daemon 140036175546112)>.\n",
      "2023-11-21 10:20:17,115\tWARNING actor_pool_map_operator.py:272 -- To ensure full parallelization across an actor pool of size 2, the Dataset should consist of at least 2 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init(num_cpus=16, num_gpus=1, log_to_driver=False) #num_gpus is the hardware count\n",
    "    ray.data.DataContext.get_current().execution_options.verbose_progress = True\n",
    "\n",
    "try:\n",
    "    ds = ray.data.from_items(chunks)\n",
    "    # ds = ray.data.from_pandas([chunks_df,])\n",
    "\n",
    "    extracts = ds.map_batches(\n",
    "        MRebelExtractor,\n",
    "        # batch_format=\"pandas\",\n",
    "        num_gpus=0.5, # per actor !!!\n",
    "        batch_size=BATCH_SIZE,\n",
    "        compute=ray.data.ActorPoolStrategy(size=2),\n",
    "        )\n",
    "    # results = extracts.take(ds.count()) # to process all\n",
    "    results = extracts.take()\n",
    "\n",
    "except Exception as inference_exc:\n",
    "    print(inference_exc)\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously pickled results\n",
    "# with open('./irsn_rebel_output.pkl', 'rb') as handle:\n",
    "#     results = pickle.load(handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>head_type</th>\n",
       "      <th>type</th>\n",
       "      <th>tail</th>\n",
       "      <th>tail_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>time</td>\n",
       "      <td>point in time</td>\n",
       "      <td>janvier 2010</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finlande</td>\n",
       "      <td>loc</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>États-Unis</td>\n",
       "      <td>loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>États-Unis</td>\n",
       "      <td>loc</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Finlande</td>\n",
       "      <td>loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Olkiluoto</td>\n",
       "      <td>loc</td>\n",
       "      <td>country</td>\n",
       "      <td>Finlande</td>\n",
       "      <td>loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>time</td>\n",
       "      <td>point in time</td>\n",
       "      <td>janvier 2010</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         head head_type                 type          tail tail_type\n",
       "0        2010      time        point in time  janvier 2010      date\n",
       "1    Finlande       loc  diplomatic relation    États-Unis       loc\n",
       "2  États-Unis       loc  diplomatic relation      Finlande       loc\n",
       "3   Olkiluoto       loc              country      Finlande       loc\n",
       "4        2010      time        point in time  janvier 2010      date"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_triplets = []\n",
    "for result in results:\n",
    "    raw_output = result['output']\n",
    "    raw_output = ' '.join(raw_output)\n",
    "    extracted_triplets.extend(extract_triplets_typed(raw_output))\n",
    "\n",
    "triplets_df = pd.DataFrame.from_dict(extracted_triplets, orient='columns')\n",
    "triplets_df.head()\n",
    "# triplets_df.groupby(['type']).size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 14838.84it/s]\n"
     ]
    }
   ],
   "source": [
    "kb = KB()\n",
    "for _, entry in tqdm(triplets_df.iterrows(), total=triplets_df.shape[0]):\n",
    "    relation = {\"head\": entry['head'], \n",
    "                \"type\":entry['type'], \n",
    "                'tail':entry['tail'],\n",
    "                \"head_type\": entry[\"head_type\"],\n",
    "                \"tail_type\": entry[\"head_type\"]}\n",
    "    kb.add_relation(relation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kg_test.html\n"
     ]
    }
   ],
   "source": [
    "filename = \"kg_test.html\"\n",
    "save_network_html(kb, filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P310_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
