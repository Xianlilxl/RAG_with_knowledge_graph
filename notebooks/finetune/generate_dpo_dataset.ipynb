{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList \n",
    "from typing import Dict, List, Generator\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "import json \n",
    "import ast\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(\n",
    "            self, \n",
    "            stop_words: List[str], \n",
    "            tokenizer: AutoTokenizer, \n",
    "            device: torch.device\n",
    "            ) -> None:\n",
    "        stop_words = [' ' + stop_word for stop_word in stop_words]\n",
    "        stop_token_ids = [tokenizer(t, add_special_tokens=False)['input_ids'][1:] for t in stop_words]\n",
    "        self.stop_token_ids = [\n",
    "            torch.LongTensor(x).to(device) for x in stop_token_ids\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "            self, \n",
    "            input_ids: torch.LongTensor, \n",
    "            scores: torch.FloatTensor, \n",
    "            **kwargs\n",
    "            ) -> bool:\n",
    "        for stop_ids in self.stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, checkpoint_dir: str) -> None:\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"THE DEVICE INFERENCE IS RUNNING ON IS: \", self.device)\n",
    "        self.tokenizer = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.stopping_criteria = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "    \n",
    "    def get_checkpoint_dir(self):\n",
    "        run = wandb.init()\n",
    "        checkpoint = run.use_artifact(self.wandb_checkpoint_name, type='model')\n",
    "        checkpoint_dir = checkpoint.download()\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,  # Mistral, same as before\n",
    "            quantization_config=QUANTIZATION_CONFIG,  # Same quantization config as before\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # load and merge the checkpoint with base model\n",
    "        self.ft_model = PeftModel.from_pretrained(self.base_model, self.checkpoint_dir)\n",
    "        self.ft_model.eval()\n",
    "\n",
    "        self.gen_cfg = GenerationConfig.from_model_config(self.ft_model.config)\n",
    "        self.gen_cfg.max_new_tokens = 1024\n",
    "        self.gen_cfg.temperature = 0.5\n",
    "        self.gen_cfg.num_return_sequences = 1\n",
    "        self.gen_cfg.use_cache = True\n",
    "        self.gen_cfg.min_length = 1\n",
    "\n",
    "    def predict(self, request: Dict) -> Dict | Generator:\n",
    "        with torch.no_grad():\n",
    "            prompt = request.pop(\"prompt\")\n",
    "            stop_words = []\n",
    "            if \"stop\" in request:\n",
    "                stop_words = request.pop(\"stop\")\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            input_ids = inputs[\"input_ids\"].cuda()\n",
    "            generation_output = self.ft_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                stopping_criteria = StoppingCriteriaList(\n",
    "                    [StopGenerationCriteria(\n",
    "                        stop_words,\n",
    "                        self.tokenizer,\n",
    "                        self.device\n",
    "                        )]),\n",
    "                generation_config=self.gen_cfg,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=256\n",
    "            )\n",
    "            outputs = []\n",
    "            for seq in generation_output.sequences:\n",
    "                output = self.tokenizer.decode(seq)\n",
    "                outputs.append(output)\n",
    "\n",
    "            return \"\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(example):\n",
    "    RELATION_NAMES=['country', 'place of birth', 'spouse', 'country of citizenship', 'instance of',\n",
    "            'capital', 'child', 'shares border with', 'author', 'director', 'occupation',\n",
    "              'founded by', 'league', 'owned by', 'genre', 'named after', 'follows',\n",
    "                'headquarters location', 'cast member', 'manufacturer',\n",
    "                  'located in or next to body of water', 'location', 'part of', \n",
    "                  'mouth of the watercourse', 'member of', 'sport', 'characters',\n",
    "                    'participant', 'notable work', 'replaces', 'sibling', 'inception']\n",
    "    relations = []\n",
    "    for relation in example['relations']:\n",
    "        relation_dict = {}\n",
    "        relation_dict[\"object\"] = relation['object']['surfaceform']\n",
    "        relation_dict[\"subject\"] = relation['subject']['surfaceform']\n",
    "        relation_dict[\"predicate\"] = RELATION_NAMES[relation['predicate']]\n",
    "        relations.append(relation_dict)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_template() -> PromptTemplate:\n",
    "    prompt_template = \"\"\"### Instruction:\n",
    "You are an expert in data science and natural language processing (NLP).\n",
    "Your task is to extract triplets from the text provided below.\n",
    "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
    "{{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }}\n",
    "Multiple triplets must be in list form.\n",
    "Text: {text}\\n\n",
    "### Response:\"\"\"\n",
    "    input_variables = [\"text\"]\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=input_variables,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(test_example):\n",
    "    prompt_template = get_prompt_template()\n",
    "\n",
    "    relations_prompt = prompt_template.format(\n",
    "        text=test_example[\"text\"],\n",
    "    )\n",
    "\n",
    "    test_example[\"prompt\"] = relations_prompt\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use checkpoint to make predictions (solar-disco-79, checkpoint-4grkql3s:v10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DEVICE INFERENCE IS RUNNING ON IS:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf87ff8687a4eb9b07ec835d88d6159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_dir = \"./checkpoints/solar-disco-79/checkpoint-4grkql3s:v10\"\n",
    "ft_model = Model(checkpoint_dir=checkpoint_dir)\n",
    "ft_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxianli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/fine-tuning/wandb/run-20231115_134112-dlogee01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xianli/uncategorized/runs/dlogee01' target=\"_blank\">flowing-energy-18</a></strong> to <a href='https://wandb.ai/xianli/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xianli/uncategorized' target=\"_blank\">https://wandb.ai/xianli/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xianli/uncategorized/runs/dlogee01' target=\"_blank\">https://wandb.ai/xianli/uncategorized/runs/dlogee01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-4grkql3s:v0, 164.08MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f2dda2a5690>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f2ddc293490, execution_count=15 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f2ddc291390, raw_cell=\"import wandb\n",
      "run = wandb.init()\n",
      "artifact = run.use..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/generate_dpo_dataset.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('xianli/digital_safety/model-4grkql3s:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the REDFM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fr_dataset = load_dataset(\"Babelscape/REDFM\", language=\"fr\", split=\"train\")\n",
    "train_en_dataset = load_dataset(\"Babelscape/REDFM\", language=\"en\", split=\"train\")\n",
    "assert train_fr_dataset.features.type == train_en_dataset.features.type\n",
    "sliced_dataset_en = train_en_dataset.shuffle(seed=42).select(\n",
    "    range(train_fr_dataset.num_rows)\n",
    ")\n",
    "concat_dataset = concatenate_datasets(\n",
    "    [sliced_dataset_en, train_fr_dataset]\n",
    ").shuffle(seed=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prompt'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset = concat_dataset.map(get_prompt)\n",
    "prompt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(test_example):\n",
    "    output = ft_model.predict(request={\n",
    "        \"prompt\": test_example[\"prompt\"],\n",
    "        \"temperature\": 0.1, \n",
    "        \"max_new_tokens\": 1024,\n",
    "        # \"stop\": [\"\\n\\n\"]\n",
    "        \"stop\": [\"\\n\\n### Instruction:\"]\n",
    "    })\n",
    "    test_example[\"prediction\"] = output\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc57f1692cd147f0b378e330c6a3485d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d2506490224dd5bf86dec7f9362822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dataset = prompt_dataset.map(get_prediction)\n",
    "pred_dataset.save_to_disk(\"./datasets/redfm_pred_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prompt', 'prediction'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset = load_from_disk(\"./datasets/redfm_pred_dataset\")\n",
    "pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(string):\n",
    "    string = string.replace(\"</s>\", \"\").replace(\"Relations: \", \"\").replace(\"\\n\", \"\")\n",
    "    relations = string.split(\"}, \")\n",
    "    relations_dict = []\n",
    "    for re in relations:\n",
    "        re = re.strip(\"[]\").strip(\"\\{\\}\")\n",
    "        re_dict = {}\n",
    "        for k_v in re.split(\", \"):\n",
    "            k, v = tuple(k_v.split(\": \"))\n",
    "            re_dict[k.replace(\"\\\"\", \"\")] = v.replace(\"\\\"\", \"\")\n",
    "        relations_dict.append(re_dict)\n",
    "    return relations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_dict(example):\n",
    "    string = example[\"prediction\"].split(\"\\n\\n### Response:\")[-1]\n",
    "    string = string.replace(\"</s>\", \"\").replace(\"Relations: \", \"\")\n",
    "    try:\n",
    "        pred_dict = json.loads(string)\n",
    "    except:\n",
    "        pred_dict = string_to_dict(string)\n",
    "    example[\"pred_dict\"] = pred_dict\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df14e7ffe4a7407d9df21039c2da42ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prompt', 'prediction', 'pred_dict'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict_dataset = pred_dataset.map(pred_to_dict)\n",
    "pred_dict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_lists_of_dicts_identical(list1, list2):\n",
    "    # Check if the lengths of the lists are the same\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    # Iterate through each dictionary in the lists and compare them\n",
    "    for dict1, dict2 in zip(list1, list2):\n",
    "        if dict1 != dict2:\n",
    "            return False\n",
    "\n",
    "    # If all dictionaries are identical, return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_and_responses(example):\n",
    "    ground_truth = get_relation(example)\n",
    "    prediction = example[\"pred_dict\"]\n",
    "    if are_lists_of_dicts_identical(ground_truth, prediction):\n",
    "        example[\"chosen\"] =  json.dumps(ground_truth, ensure_ascii=False)\n",
    "        example[\"rejected\"] = \"\"\n",
    "    else:\n",
    "        example[\"chosen\"] =  json.dumps(ground_truth, ensure_ascii=False)\n",
    "        example[\"rejected\"] = json.dumps(prediction, ensure_ascii=False)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Object': 'Loki', 'Predicate': 'parent', 'Subject': 'Jörmungand'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(return_prompt_and_responses(pred_dict_dataset[10])[\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Object': 'Rouen', 'Predicate': 'capital', 'Subject': 'Normandy'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(return_prompt_and_responses(pred_dict_dataset[0])[\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbdf289335743189080f1de8bcf4c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prompt', 'prediction', 'pred_dict', 'chosen', 'rejected'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = pred_dict_dataset.map(return_prompt_and_responses)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docid': '86182-1',\n",
       " 'title': 'Jörmungand',\n",
       " 'uri': 'Q181227',\n",
       " 'text': 'Selon l’\"Edda de Snorri\", il est le fils du dieu malin Loki et de la géante Angrboda, et le frère du loup Fenrir ainsi que de la déesse du monde des morts Hel. Peu après sa naissance, le dieu Odin jette Jörmungand dans la mer qui encercle Midgard, puisque les prophéties racontent qu\\'il causera de grands dégâts chez les dieux. Mais ce dernier grandit tellement qu\\'il finit par entourer le monde et se mordre la queue, d\\'où son autre nom, Midgardsorm \"(Miðgarðsormr)\", .',\n",
       " 'entities': [{'uri': 'Q205882',\n",
       "   'surfaceform': 'Edda de Snorri',\n",
       "   'type': 'MEDIA',\n",
       "   'start': 9,\n",
       "   'end': 23},\n",
       "  {'uri': 'Q133147',\n",
       "   'surfaceform': 'Loki',\n",
       "   'type': 'MISC',\n",
       "   'start': 55,\n",
       "   'end': 59},\n",
       "  {'uri': 'Q210053',\n",
       "   'surfaceform': 'géante',\n",
       "   'type': 'Concept',\n",
       "   'start': 69,\n",
       "   'end': 75},\n",
       "  {'uri': 'Q371828',\n",
       "   'surfaceform': 'Angrboda',\n",
       "   'type': 'MISC',\n",
       "   'start': 76,\n",
       "   'end': 84},\n",
       "  {'uri': 'Q182560',\n",
       "   'surfaceform': 'Fenrir',\n",
       "   'type': 'MISC',\n",
       "   'start': 106,\n",
       "   'end': 112},\n",
       "  {'uri': 'Q191589',\n",
       "   'surfaceform': 'Hel',\n",
       "   'type': 'MISC',\n",
       "   'start': 155,\n",
       "   'end': 158},\n",
       "  {'uri': 'Q43610',\n",
       "   'surfaceform': 'Odin',\n",
       "   'type': 'MISC',\n",
       "   'start': 192,\n",
       "   'end': 196},\n",
       "  {'uri': 'Q181227',\n",
       "   'surfaceform': 'Jörmungand',\n",
       "   'type': 'MISC',\n",
       "   'start': 203,\n",
       "   'end': 213},\n",
       "  {'uri': 'Q180682',\n",
       "   'surfaceform': 'Midgard',\n",
       "   'type': 'Concept',\n",
       "   'start': 239,\n",
       "   'end': 246}],\n",
       " 'relations': [{'subject': {'uri': 'Q133147',\n",
       "    'surfaceform': 'Loki',\n",
       "    'type': 'MISC',\n",
       "    'start': 55,\n",
       "    'end': 59},\n",
       "   'predicate': 24,\n",
       "   'object': {'uri': 'Q210053',\n",
       "    'surfaceform': 'géante',\n",
       "    'type': 'Concept',\n",
       "    'start': 69,\n",
       "    'end': 75}},\n",
       "  {'subject': {'uri': 'Q133147',\n",
       "    'surfaceform': 'Loki',\n",
       "    'type': 'MISC',\n",
       "    'start': 55,\n",
       "    'end': 59},\n",
       "   'predicate': 6,\n",
       "   'object': {'uri': 'Q182560',\n",
       "    'surfaceform': 'Fenrir',\n",
       "    'type': 'MISC',\n",
       "    'start': 106,\n",
       "    'end': 112}},\n",
       "  {'subject': {'uri': 'Q133147',\n",
       "    'surfaceform': 'Loki',\n",
       "    'type': 'MISC',\n",
       "    'start': 55,\n",
       "    'end': 59},\n",
       "   'predicate': 6,\n",
       "   'object': {'uri': 'Q191589',\n",
       "    'surfaceform': 'Hel',\n",
       "    'type': 'MISC',\n",
       "    'start': 155,\n",
       "    'end': 158}},\n",
       "  {'subject': {'uri': 'Q371828',\n",
       "    'surfaceform': 'Angrboda',\n",
       "    'type': 'MISC',\n",
       "    'start': 76,\n",
       "    'end': 84},\n",
       "   'predicate': 24,\n",
       "   'object': {'uri': 'Q210053',\n",
       "    'surfaceform': 'géante',\n",
       "    'type': 'Concept',\n",
       "    'start': 69,\n",
       "    'end': 75}},\n",
       "  {'subject': {'uri': 'Q371828',\n",
       "    'surfaceform': 'Angrboda',\n",
       "    'type': 'MISC',\n",
       "    'start': 76,\n",
       "    'end': 84},\n",
       "   'predicate': 6,\n",
       "   'object': {'uri': 'Q182560',\n",
       "    'surfaceform': 'Fenrir',\n",
       "    'type': 'MISC',\n",
       "    'start': 106,\n",
       "    'end': 112}},\n",
       "  {'subject': {'uri': 'Q371828',\n",
       "    'surfaceform': 'Angrboda',\n",
       "    'type': 'MISC',\n",
       "    'start': 76,\n",
       "    'end': 84},\n",
       "   'predicate': 6,\n",
       "   'object': {'uri': 'Q191589',\n",
       "    'surfaceform': 'Hel',\n",
       "    'type': 'MISC',\n",
       "    'start': 155,\n",
       "    'end': 158}},\n",
       "  {'subject': {'uri': 'Q371828',\n",
       "    'surfaceform': 'Angrboda',\n",
       "    'type': 'MISC',\n",
       "    'start': 76,\n",
       "    'end': 84},\n",
       "   'predicate': 6,\n",
       "   'object': {'uri': 'Q181227',\n",
       "    'surfaceform': 'Jörmungand',\n",
       "    'type': 'MISC',\n",
       "    'start': 203,\n",
       "    'end': 213}},\n",
       "  {'subject': {'uri': 'Q182560',\n",
       "    'surfaceform': 'Fenrir',\n",
       "    'type': 'MISC',\n",
       "    'start': 106,\n",
       "    'end': 112},\n",
       "   'predicate': 30,\n",
       "   'object': {'uri': 'Q191589',\n",
       "    'surfaceform': 'Hel',\n",
       "    'type': 'MISC',\n",
       "    'start': 155,\n",
       "    'end': 158}},\n",
       "  {'subject': {'uri': 'Q191589',\n",
       "    'surfaceform': 'Hel',\n",
       "    'type': 'MISC',\n",
       "    'start': 155,\n",
       "    'end': 158},\n",
       "   'predicate': 24,\n",
       "   'object': {'uri': 'Q210053',\n",
       "    'surfaceform': 'géante',\n",
       "    'type': 'Concept',\n",
       "    'start': 69,\n",
       "    'end': 75}}],\n",
       " 'prompt': '### Instruction:\\nYou are an expert in data science and natural language processing (NLP).\\nYour task is to extract triplets from the text provided below.\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triplets must be in list form.\\nText: Selon l’\"Edda de Snorri\", il est le fils du dieu malin Loki et de la géante Angrboda, et le frère du loup Fenrir ainsi que de la déesse du monde des morts Hel. Peu après sa naissance, le dieu Odin jette Jörmungand dans la mer qui encercle Midgard, puisque les prophéties racontent qu\\'il causera de grands dégâts chez les dieux. Mais ce dernier grandit tellement qu\\'il finit par entourer le monde et se mordre la queue, d\\'où son autre nom, Midgardsorm \"(Miðgarðsormr)\", .\\n\\n### Response:',\n",
       " 'prediction': '<s> ### Instruction:\\nYou are an expert in data science and natural language processing (NLP).\\nYour task is to extract triplets from the text provided below.\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triplets must be in list form.\\nText: Selon l’\"Edda de Snorri\", il est le fils du dieu malin Loki et de la géante Angrboda, et le frère du loup Fenrir ainsi que de la déesse du monde des morts Hel. Peu après sa naissance, le dieu Odin jette Jörmungand dans la mer qui encercle Midgard, puisque les prophéties racontent qu\\'il causera de grands dégâts chez les dieux. Mais ce dernier grandit tellement qu\\'il finit par entourer le monde et se mordre la queue, d\\'où son autre nom, Midgardsorm \"(Miðgarðsormr)\", .\\n\\n### Response:\\nRelations: [{\"Object\": \"Loki\", \"Predicate\": \"parent\", \"Subject\": \"Jörmungand\"}]</s>',\n",
       " 'pred_dict': [{'Object': 'Loki',\n",
       "   'Predicate': 'parent',\n",
       "   'Subject': 'Jörmungand'}],\n",
       " 'chosen': '[{\"object\": \"géante\", \"subject\": \"Loki\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"géante\", \"subject\": \"Angrboda\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Jörmungand\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Fenrir\", \"predicate\": \"sibling\"}, {\"object\": \"géante\", \"subject\": \"Hel\", \"predicate\": \"member of\"}]',\n",
       " 'rejected': '[{\"Object\": \"Loki\", \"Predicate\": \"parent\", \"Subject\": \"Jörmungand\"}]'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d956232b96614f178ea524f66446a8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prompt', 'prediction', 'pred_dict', 'chosen', 'rejected'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset.filter(lambda example: example[\"rejected\"]!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 3730\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = dpo_dataset.remove_columns(['docid', 'title', 'uri', 'text', 'entities', 'relations', 'prediction', 'pred_dict'])\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '### Instruction:\\nYou are an expert in data science and natural language processing (NLP).\\nYour task is to extract triplets from the text provided below.\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triplets must be in list form.\\nText: Selon l’\"Edda de Snorri\", il est le fils du dieu malin Loki et de la géante Angrboda, et le frère du loup Fenrir ainsi que de la déesse du monde des morts Hel. Peu après sa naissance, le dieu Odin jette Jörmungand dans la mer qui encercle Midgard, puisque les prophéties racontent qu\\'il causera de grands dégâts chez les dieux. Mais ce dernier grandit tellement qu\\'il finit par entourer le monde et se mordre la queue, d\\'où son autre nom, Midgardsorm \"(Miðgarðsormr)\", .\\n\\n### Response:',\n",
       " 'chosen': '[{\"object\": \"géante\", \"subject\": \"Loki\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"géante\", \"subject\": \"Angrboda\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Jörmungand\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Fenrir\", \"predicate\": \"sibling\"}, {\"object\": \"géante\", \"subject\": \"Hel\", \"predicate\": \"member of\"}]',\n",
       " 'rejected': '[{\"Object\": \"Loki\", \"Predicate\": \"parent\", \"Subject\": \"Jörmungand\"}]'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae872670a02446908f0e98d9eeac6063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 3079\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = dpo_dataset.filter(lambda example: len(example[\"prompt\"]) < 1024, load_from_cache_file=False)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158671337a9c4f38b8e4190ea586c607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_dataset.save_to_disk(\"./datasets/dpo_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    is_trainable=True,\n",
    ")\n",
    "model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "...\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=script_args.beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '### Instruction:\\nYou are an expert in data science and natural language processing (NLP).\\nYour task is to extract triplets from the text provided below.\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triplets must be in list form.\\nText: Selon l’\"Edda de Snorri\", il est le fils du dieu malin Loki et de la géante Angrboda, et le frère du loup Fenrir ainsi que de la déesse du monde des morts Hel. Peu après sa naissance, le dieu Odin jette Jörmungand dans la mer qui encercle Midgard, puisque les prophéties racontent qu\\'il causera de grands dégâts chez les dieux. Mais ce dernier grandit tellement qu\\'il finit par entourer le monde et se mordre la queue, d\\'où son autre nom, Midgardsorm \"(Miðgarðsormr)\", .\\n\\n### Response:',\n",
       " 'chosen': '[{\"object\": \"géante\", \"subject\": \"Loki\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Loki\", \"predicate\": \"child\"}, {\"object\": \"géante\", \"subject\": \"Angrboda\", \"predicate\": \"member of\"}, {\"object\": \"Fenrir\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Jörmungand\", \"subject\": \"Angrboda\", \"predicate\": \"child\"}, {\"object\": \"Hel\", \"subject\": \"Fenrir\", \"predicate\": \"sibling\"}, {\"object\": \"géante\", \"subject\": \"Hel\", \"predicate\": \"member of\"}]',\n",
       " 'rejected': '[{\"Object\": \"Loki\", \"Predicate\": \"parent\", \"Subject\": \"Jörmungand\"}]'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = load_from_disk(\".//datasets/dpo_dataset\")\n",
    "dpo_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_venv",
   "language": "python",
   "name": "finetune_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
