{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList \n",
    "from typing import Dict, List, Generator\n",
    "from datasets import load_dataset, load_from_disk\n",
    "# from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "import json \n",
    "import ast\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(\n",
    "            self, \n",
    "            stop_words: List[str], \n",
    "            tokenizer: AutoTokenizer, \n",
    "            device: torch.device\n",
    "            ) -> None:\n",
    "        stop_words = [' ' + stop_word for stop_word in stop_words]\n",
    "        stop_token_ids = [tokenizer(t, add_special_tokens=False)['input_ids'][1:] for t in stop_words]\n",
    "        self.stop_token_ids = [\n",
    "            torch.LongTensor(x).to(device) for x in stop_token_ids\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "            self, \n",
    "            input_ids: torch.LongTensor, \n",
    "            scores: torch.FloatTensor, \n",
    "            **kwargs\n",
    "            ) -> bool:\n",
    "        for stop_ids in self.stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, checkpoint_dir: str) -> None:\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"THE DEVICE INFERENCE IS RUNNING ON IS: \", self.device)\n",
    "        self.tokenizer = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.stopping_criteria = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "    \n",
    "    def get_checkpoint_dir(self):\n",
    "        run = wandb.init()\n",
    "        checkpoint = run.use_artifact(self.wandb_checkpoint_name, type='model')\n",
    "        checkpoint_dir = checkpoint.download()\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,  # Mistral, same as before\n",
    "            quantization_config=QUANTIZATION_CONFIG,  # Same quantization config as before\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # load and merge the checkpoint with base model\n",
    "        self.ft_model = PeftModel.from_pretrained(self.base_model, self.checkpoint_dir)\n",
    "        self.ft_model.eval()\n",
    "\n",
    "        self.gen_cfg = GenerationConfig.from_model_config(self.ft_model.config)\n",
    "        self.gen_cfg.max_new_tokens = 1024\n",
    "        self.gen_cfg.temperature = 0.5\n",
    "        self.gen_cfg.num_return_sequences = 1\n",
    "        self.gen_cfg.use_cache = True\n",
    "        self.gen_cfg.min_length = 1\n",
    "\n",
    "    def predict(self, request: Dict) -> Dict | Generator:\n",
    "        with torch.no_grad():\n",
    "            prompt = request.pop(\"prompt\")\n",
    "            stop_words = []\n",
    "            if \"stop\" in request:\n",
    "                stop_words = request.pop(\"stop\")\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            input_ids = inputs[\"input_ids\"].cuda()\n",
    "            generation_output = self.ft_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                stopping_criteria = StoppingCriteriaList(\n",
    "                    [StopGenerationCriteria(\n",
    "                        stop_words,\n",
    "                        self.tokenizer,\n",
    "                        self.device\n",
    "                        )]),\n",
    "                generation_config=self.gen_cfg,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=256\n",
    "            )\n",
    "            outputs = []\n",
    "            for seq in generation_output.sequences:\n",
    "                output = self.tokenizer.decode(seq)\n",
    "                outputs.append(output)\n",
    "\n",
    "            return \"\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get the testing prompt and ground truth into a list of dictionaries for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(example: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and structures relations from a single example within a English dataset.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing entities and relations.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representation of the extracted relations.\n",
    "\n",
    "    Example:\n",
    "        Given an 'example' dictionary containing 'entities' and 'relations', this function\n",
    "        extracts and structures relations, returning them as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    entities_ls = example[\"entities\"]\n",
    "    relations = []\n",
    "    for relation in example[\"relations\"]:\n",
    "        relation_dict = {}\n",
    "        object_index = relation[\"object\"]\n",
    "        relation_dict[\"Object\"] = entities_ls[object_index][\"surfaceform\"]\n",
    "        relation_dict[\"Predicate\"] = relation[\"predicate\"]\n",
    "        subject_index = relation[\"subject\"]\n",
    "        relation_dict[\"Subject\"] = entities_ls[subject_index][\"surfaceform\"]\n",
    "        relations.append(relation_dict)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(example: Dict) -> List:\n",
    "    # Extract the list of entities' surface forms without duplicates\n",
    "    entities = example[\"entities\"]\n",
    "    unique_entities_surface_forms = list(\n",
    "        set(entity[\"surfaceform\"] for entity in entities)\n",
    "    )\n",
    "    return unique_entities_surface_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(test_example):\n",
    "    # try:\n",
    "    #     prompt, _ = tuple(test_example[\"text\"].split(\"### ENTITES:\"))\n",
    "    # except:\n",
    "    #     prompt, _ = tuple(test_example[\"text\"].split(\"### ENTITIES:\"))\n",
    "    # prompt+=\"### RELATIONS:\\n\"\n",
    "    # test_example[\"prompt\"] = prompt\n",
    "    test_example[\"entities_GT\"]=get_entities(test_example)\n",
    "    test_example[\"relations_GT\"]=get_relation(test_example)\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_prompt_template():\n",
    "    prompt_template = \"\"\"### Instruction:\n",
    "You are an expert in data science and natural language processing (NLP).\n",
    "Your task is to extract entities from the text provided below.\n",
    "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
    "['entity1', 'entity2', 'entity3', ...]\n",
    "Text: {text}\\n\n",
    "### Response:\"\"\"\n",
    "    input_variables = [\"text\"]\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=input_variables,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_prompt_template():\n",
    "    prompt_template = \"\"\"### Instruction:\n",
    "You are an expert in data science and natural language processing (NLP).\n",
    "Your task is to extract triplets from the text provided below.\n",
    "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
    "{{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }}\n",
    "Multiple triplets must be in list form.\n",
    "Text: {text}\\n\n",
    "### Response:\"\"\"\n",
    "    input_variables = [\"text\"]\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=input_variables,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(test_example):\n",
    "    text = test_example[\"text\"].split(\"\\nText: \")[1].split(\"\\n\\n### Response:\")[0]\n",
    "\n",
    "    entities_prompt_template = get_entities_prompt_template()\n",
    "    relations_prompt_template = get_relations_prompt_template()\n",
    "    # Create the full prompt by filling in the template\n",
    "    entities_prompt = entities_prompt_template.format(\n",
    "        text=text,\n",
    "    )\n",
    "    relations_prompt = relations_prompt_template.format(\n",
    "        text=text,\n",
    "    )\n",
    "\n",
    "    test_example[\"entities_prompt\"] = entities_prompt\n",
    "    test_example[\"relations_prompt\"] = relations_prompt\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in solar-serenity-74\n",
    "def entities_relation_GT(test_example):\n",
    "    prompt, _ = tuple(test_example[\"text\"].split(\"### Response:\"))\n",
    "    prompt += \"### Response:\\n\"\n",
    "    test_example[\"prompt\"] = prompt\n",
    "    test_example[\"entities_GT\"] = get_entities(test_example)\n",
    "    test_example[\"relation_GT\"] = get_relation(test_example)\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get prediction result based previsouly extracted prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(test_example):\n",
    "    output = ft_model.predict(request={\n",
    "        \"prompt\": test_example[\"relations_prompt\"],\n",
    "        \"temperature\": 0.1, \n",
    "        \"max_new_tokens\": 1024,\n",
    "        # \"stop\": [\"\\n\\n\"]\n",
    "        \"stop\": [\"\\n\\n### Instruction:\"]\n",
    "    })\n",
    "    # output_dict = string_to_dict(output)\n",
    "    test_example[\"prediction\"] = output\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. load relation prediction into list of dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(test_example):\n",
    "    string = test_example[\"prediction\"]\n",
    "    string = string.split(\"### RELATIONS:\\n\")[1]\n",
    "    string = string.replace(\"</s>\", \"\").replace(\"\\n\", \"\")\n",
    "    \n",
    "        #     try:\n",
    "        #         k, v = tuple(k_v.split(\"': \"))\n",
    "        #     except:\n",
    "        #         print(k_v)\n",
    "        #     relation[k.strip(\"'\\\" \")] = v.strip(\"'\\\" \")\n",
    "        # extracted_relations.append(relation)\n",
    "\n",
    "    test_example[\"prediction_dict\"] = json.loads(string)\n",
    "    return test_example   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GT_string_to_dict(test_example):\n",
    "\n",
    "    test_example[\"ground_truth\"] = json.loads(test_example[\"ground_truth\"])\n",
    "    return test_example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. count true positive, ground truth and prediction to calculate f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prediction(test_example):\n",
    "    string = test_example['prediction'].split(\"### Response:\\nRelations: \")[-1].replace(\"</s>\", \"\").replace(\"\\n\", \"\")\n",
    "    prediction_dict = json.loads(string)\n",
    "    test_example[\"prediction_dict\"] = prediction_dict\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_positive(test_example):\n",
    "    # gt_ls = test_example[\"ground_truth\"]\n",
    "    gt_ls = test_example[\"relations_GT\"]\n",
    "    pred_ls = test_example[\"prediction_dict\"]\n",
    "    true_positive = 0\n",
    "    for pred in pred_ls:\n",
    "        if pred in gt_ls:\n",
    "            true_positive+=1 \n",
    "    test_example[\"correct\"] = true_positive\n",
    "    # test_example[\"guess\"] = len(test_example[\"ground_truth\"])\n",
    "    test_example[\"guess\"] = len(test_example[\"relations_GT\"])\n",
    "    test_example[\"gold\"] = len(test_example[\"prediction_dict\"])\n",
    "    return test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoints and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pious-sound-65 run\n",
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/checkpoint-17x5m17w:v10', type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/SREDFM-dataset:v3', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bright-shape-68 run\n",
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/checkpoint-o04konfu:v10', type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solar-disco-79 run (r=16, alpha=64, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/checkpoint-wvo5zep6:v18', type='model')\n",
    "# artifact_dir = artifact.download(root=\"./checkpoints/solar-disco-79/checkpoint-4grkql3s:v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/SREDFM-dataset:v12', type='dataset')\n",
    "# artifact_dir = artifact.download(root=\"./datasets/solar-disco-79/SREDFM-dataset:v12/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint-4grkql3s:v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DEVICE INFERENCE IS RUNNING ON IS:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2efc645b804c70b9673ff37dc55af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_dir = \"./checkpoints/solar-disco-79/checkpoint-4grkql3s:v10\"\n",
    "ft_model = Model(checkpoint_dir=checkpoint_dir)\n",
    "ft_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7648002b078d4073a09941e90e9fd81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0b11b557604114805a0eba1bc90dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/fine-tuning/test_checkpoints.ipynb Cellule 32\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m load_from_disk(dataset_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m prompt_dataset \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mmap(get_prompt)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pred_dataset \u001b[39m=\u001b[39m prompt_dataset\u001b[39m.\u001b[39;49mmap(get_prediction)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pred_dataset\u001b[39m.\u001b[39msave_to_disk(\u001b[39m'\u001b[39m\u001b[39m./datasets/eager-rain-77/pred_dataset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m GT_pred_dataset \u001b[39m=\u001b[39m pred_dataset\u001b[39m.\u001b[39mmap(get_ground_truth)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;49;00m rank, done, content \u001b[39min\u001b[39;49;00m Dataset\u001b[39m.\u001b[39;49m_map_single(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;49;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/home/ubuntu/fine-tuning/test_checkpoints.ipynb Cellule 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_prediction\u001b[39m(test_example):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     output \u001b[39m=\u001b[39m ft_model\u001b[39m.\u001b[39;49mpredict(request\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: test_example[\u001b[39m\"\u001b[39;49m\u001b[39mrelations_prompt\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.1\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# \"stop\": [\"\\n\\n\"]\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m### Instruction:\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     })\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# output_dict = string_to_dict(output)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     test_example[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output\n",
      "\u001b[1;32m/home/ubuntu/fine-tuning/test_checkpoints.ipynb Cellule 32\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m input_ids \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m generation_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mft_model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     stopping_criteria \u001b[39m=\u001b[39;49m StoppingCriteriaList(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m         [StopGenerationCriteria(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m             stop_words,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m             )]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     generation_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_cfg,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     output_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/test_checkpoints.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m generation_output\u001b[39m.\u001b[39msequences:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/peft/peft_model.py:1022\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m   1021\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1023\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1647\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1631\u001b[0m         input_ids,\n\u001b[1;32m   1632\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1644\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1646\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1648\u001b[0m         input_ids,\n\u001b[1;32m   1649\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1650\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1651\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1652\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1653\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1654\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1655\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1656\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1657\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1658\u001b[0m     )\n\u001b[1;32m   1660\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1661\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/generation/utils.py:2495\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2494\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2495\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2496\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2497\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2498\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2499\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2500\u001b[0m )\n\u001b[1;32m   2502\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2503\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1041\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1043\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1044\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1045\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1046\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1047\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1048\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1049\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1050\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1051\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1052\u001b[0m )\n\u001b[1;32m   1054\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1055\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:929\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    923\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    924\u001b[0m         hidden_states,\n\u001b[1;32m    925\u001b[0m         attention_mask,\n\u001b[1;32m    926\u001b[0m         position_ids,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    930\u001b[0m         hidden_states,\n\u001b[1;32m    931\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    932\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    933\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    934\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    935\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    936\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    937\u001b[0m     )\n\u001b[1;32m    939\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    941\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:618\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    615\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    617\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    619\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    620\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    621\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    622\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    623\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    624\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    625\u001b[0m     padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    626\u001b[0m )\n\u001b[1;32m    627\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    629\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:258\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    248\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m     padding_mask: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    255\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor], Optional[Tuple[torch\u001b[39m.\u001b[39mTensor]]]:\n\u001b[1;32m    256\u001b[0m     bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[0;32m--> 258\u001b[0m     query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states)\n\u001b[1;32m    259\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    260\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:269\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_layer\u001b[39m.\u001b[39mforward(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_layer\u001b[39m.\u001b[39;49mforward(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[39m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39m# sure.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul_4bit(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mt(), bias\u001b[39m=\u001b[39;49mbias, quant_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mquant_state)\n\u001b[1;32m    250\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39;49mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mlinear(A, F\u001b[39m.\u001b[39;49mdequantize_4bit(B, state)\u001b[39m.\u001b[39;49mto(A\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataset_path = \"./datasets/solar-disco-79/SREDFM-dataset:v12/test/\"\n",
    "dataset_path = \"./datasets/SREDFM-dataset-1024/test/\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "prompt_dataset = test_dataset.map(get_prompt)\n",
    "pred_dataset = prompt_dataset.map(get_prediction)\n",
    "pred_dataset.save_to_disk('./datasets/eager-rain-77/pred_dataset')\n",
    "GT_pred_dataset = pred_dataset.map(get_ground_truth)\n",
    "GT_pred_dict_dataset = GT_pred_dataset.map(parse_prediction)\n",
    "metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "correct = sum(metrics_dataset[\"correct\"])\n",
    "guess = sum(metrics_dataset[\"guess\"])\n",
    "gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "precision = float(correct)/float(guess)\n",
    "recall = float(correct)/float(gold)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint-4grkql3s:v18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoints/solar-disco-79/checkpoint-4grkql3s:v18\"\n",
    "ft_model = Model(checkpoint_dir=checkpoint_dir)\n",
    "ft_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/solar-disco-79/SREDFM-dataset:v12/test/\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "prompt_dataset = test_dataset.map(get_prompt)\n",
    "pred_dataset = prompt_dataset.map(get_prediction)\n",
    "pred_dataset.save_to_disk('./datasets/eager-rain-77/pred_dataset')\n",
    "GT_pred_dataset = pred_dataset.map(get_ground_truth)\n",
    "GT_pred_dict_dataset = GT_pred_dataset.map(parse_prediction)\n",
    "metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "correct = sum(metrics_dataset[\"correct\"])\n",
    "guess = sum(metrics_dataset[\"guess\"])\n",
    "gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "precision = float(correct)/float(guess)\n",
    "recall = float(correct)/float(gold)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eager-rain-77 run (changed the prompt into dialogue with packing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/checkpoint-wvo5zep6:v10', type='model')\n",
    "# artifact_dir = artifact.download(root=\"./checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1692767e30349ea9b5f5c033f5849a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113743588888761, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_9610/1890665174.py 2 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xli/Documents/Digital safety/notebooks/finetune/test_checkpoints.ipynb Cellule 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/notebooks/finetune/test_checkpoints.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mWANDB_BASE_URL\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://api.wandb.ai\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/notebooks/finetune/test_checkpoints.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/notebooks/finetune/test_checkpoints.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m artifact \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39muse_artifact(\u001b[39m'\u001b[39m\u001b[39mxianli/digital_safety/SREDFM-dataset:v11\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/notebooks/finetune/test_checkpoints.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m artifact_dir \u001b[39m=\u001b[39m artifact\u001b[39m.\u001b[39mdownload(root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./datasets/eager-rain-77/SREDFM-dataset:v11/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1188\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1187\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1188\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1189\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1190\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1165\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1163\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1166\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1167\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:751\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommunicating run to backend with \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    750\u001b[0m run_init_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 751\u001b[0m result \u001b[39m=\u001b[39m run_init_handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    752\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    753\u001b[0m     on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init,\n\u001b[1;32m    754\u001b[0m     cancel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    757\u001b[0m     run_result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mrun_result\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/SREDFM-dataset:v11', type='dataset')\n",
    "# artifact_dir = artifact.download(root=\"./datasets/eager-rain-77/SREDFM-dataset:v11/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DEVICE INFERENCE IS RUNNING ON IS:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f0838f21f8471ebfd16f25b1e88931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5734befe980f4a0db066781238603649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0daca95c0ff0477dac0d938bb2af59b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596825196d9d448fa4102083a74ad003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f3bfec2b4345aebff156ac7d868ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9fc299a971486a872736d0da5471ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b58d27c8c6b43ce98e96d46b14a5a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad4ed6c2b654f65a7368c8ce9044003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xli/Documents/Digital safety/src/notebooks/finetune/test_checkpoints.ipynb Cellule 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m artifact_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./checkpoints/eager-rain-77/checkpoint-wvo5zep6:v10\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ft_model \u001b[39m=\u001b[39m Model(checkpoint_dir\u001b[39m=\u001b[39martifact_dir)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m ft_model\u001b[39m.\u001b[39;49mload()\n",
      "\u001b[1;32m/home/xli/Documents/Digital safety/src/notebooks/finetune/test_checkpoints.ipynb Cellule 30\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(BASE_MODEL)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     BASE_MODEL,  \u001b[39m# Mistral, same as before\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mQUANTIZATION_CONFIG,  \u001b[39m# Same quantization config as before\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# torch_dtype=torch.bfloat16,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# load and merge the checkpoint with base model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mft_model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3002\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2999\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3000\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3001\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3002\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3003\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3004\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3005\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   3006\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   3007\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   3008\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   3009\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   3010\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3011\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   3012\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   3013\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   3014\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   3015\u001b[0m     )\n\u001b[1;32m   3017\u001b[0m \u001b[39m# load pt weights early so that we know which dtype to init the model under\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/utils/hub.py:1040\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1038\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1041\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1042\u001b[0m             shard_filename,\n\u001b[1;32m   1043\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1044\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1045\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1046\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1047\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1048\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1049\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1050\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1051\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1052\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m   1053\u001b[0m         )\n\u001b[1;32m   1054\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_venv/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "artifact_dir = \"./checkpoints/eager-rain-77/checkpoint-wvo5zep6:v10\"\n",
    "ft_model = Model(checkpoint_dir=artifact_dir)\n",
    "ft_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Instruction:\n",
       "You are an expert in data science and natural language processing (NLP).\n",
       "Your task is to extract entities from the text provided below.\n",
       "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
       "['entity1', 'entity2', 'entity3', ...]\n",
       "Text: La Grosse Combine (titre original : \"\") est un film franco-italien réalisé par Bruno Corbucci et sorti en 1971.\n",
       "\n",
       "### Response:\n",
       "Entities: [\"Bruno Corbucci\", \"film franco\", \"1971\", \"italien\", \"La Grosse Combine\"]</s>\n",
       "\n",
       "### Instruction:\n",
       "Now based on the entities that you extracted before, you should extract all triplets including every extracted entity.\n",
       "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
       "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
       "Entities can be related to many other entities.\n",
       "Multiple triplets must be in list form.\n",
       "\n",
       "### Response:\n",
       "Relations: [{\"Object\": \"Bruno Corbucci\", \"Predicate\": \"director\", \"Subject\": \"La Grosse Combine\"}]</s>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"../../artifacts/datasets/SREDFM-dataset-1024/test\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "display(Markdown(test_dataset[2][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Create a classification task by clustering the given list of items.\n",
       "\n",
       "### Input:\n",
       "Apples, oranges, bananas, strawberries, pineapples\n",
       "\n",
       "### Response:\n",
       "Class 1: Apples, Oranges\n",
       "Class 2: Bananas, Strawberries\n",
       "Class 3: Pineapples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a classification task by clustering the given list of items.\\n\\n### Input:\\nApples, oranges, bananas, strawberries, pineapples\\n\\n### Response:\\nClass 1: Apples, Oranges\\nClass 2: Bananas, Strawberries\\nClass 3: Pineapples\"\n",
    "display(Markdown(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Instruction:\n",
       "You are an expert in data science and natural language processing (NLP).\n",
       "Your task is to extract entities from the text provided below.\n",
       "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
       "['entity1', 'entity2', 'entity3', ...]\n",
       "Text: La Grosse Combine (titre original : \"\") est un film franco-italien réalisé par Bruno Corbucci et sorti en 1971.\n",
       "\n",
       "### Response:\n",
       "Entities: [\"Bruno Corbucci\", \"1971\", \"italien\", \"film franco\", \"La Grosse Combine\"]</s>\n",
       "\n",
       "### Instruction:\n",
       "You are an expert in data science and natural language processing (NLP).\n",
       "Your task is to extract triplets from the text provided below.\n",
       "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
       "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
       "Multiple triplets must be in list form.\n",
       "\n",
       "### Response:\n",
       "Relations: [{\"Object\": \"Bruno Corbucci\", \"Predicate\": \"director\", \"Subject\": \"La Grosse Combine\"}]</s>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"../../artifacts/datasets/eager-rain-77/SREDFM-dataset:v11/test\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "display(Markdown(test_dataset[2][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = test_dataset.map(get_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an expert in data science and natural language processing (NLP).\n",
      "Your task is to extract entities from the text provided below.\n",
      "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
      "['entity1', 'entity2', 'entity3', ...]\n",
      "Text: Khattiya Sawasdiphol (thaï : พลตรี ขัตติยะ สวัสดิผล), alias Seh Daeng (thaï : เสธ. แดง ; français : « Le commandant rouge »), né le et tué le , est un général de division de l'armée thaïlandaise, autrefois affecté au commandement des opérations de sécurité interne. \n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_dataset[0][\"entities_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a2df5af0844dd6afb0bf961230e659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:363: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GT_pred_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xli/Documents/Digital safety/src/notebooks/finetune/test_checkpoints.ipynb Cellule 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pred_dataset \u001b[39m=\u001b[39m prompt_dataset\u001b[39m.\u001b[39mmap(get_prediction)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xli/Documents/Digital%20safety/src/notebooks/finetune/test_checkpoints.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m GT_pred_dataset\u001b[39m.\u001b[39msave_to_disk(\u001b[39m'\u001b[39m\u001b[39m./datasets/eager-rain-77/pred_dataset\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GT_pred_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "pred_dataset = prompt_dataset.map(get_prediction)\n",
    "pred_dataset.save_to_disk('./datasets/eager-rain-77/pred_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'entities_prompt', 'relations_prompt', 'prediction'],\n",
       "    num_rows: 2529\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset = load_from_disk('./datasets/eager-rain-77/pred_dataset')\n",
    "pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'entities_prompt', 'relations_prompt', 'prediction', 'entities_GT', 'relations_GT'],\n",
       "    num_rows: 2529\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GT_pred_dataset = pred_dataset.map(get_ground_truth)\n",
    "GT_pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction:\n",
      "You are an expert in data science and natural language processing (NLP).\n",
      "Your task is to extract triplets from the text provided below.\n",
      "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
      "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
      "Multiple triplets must be in list form.\n",
      "Text: Khattiya Sawasdiphol (thaï : พลตรี ขัตติยะ สวัสดิผล), alias Seh Daeng (thaï : เสธ. แดง ; français : « Le commandant rouge »), né le et tué le , est un général de division de l'armée thaïlandaise, autrefois affecté au commandement des opérations de sécurité interne. \n",
      "\n",
      "### Response:\n",
      "Relations: [{\"Object\": \"thaïlandaise\", \"Predicate\": \"country of citizenship\", \"Subject\": \"Khattiya Sawasdiphol\"}]</s>\n"
     ]
    }
   ],
   "source": [
    "print(GT_pred_dataset[0][\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da69fc1b2df044b7afb21e1a3dda82d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations', 'entities_prompt', 'relations_prompt', 'prediction', 'entities_GT', 'relations_GT', 'prediction_dict'],\n",
       "    num_rows: 2529\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GT_pred_dict_dataset = GT_pred_dataset.map(parse_prediction)\n",
    "GT_pred_dict_dataset.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c49231d62d945c6a69dc504fee65e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37620849096258935 0.581924577373212 0.45698238447791684\n"
     ]
    }
   ],
   "source": [
    "metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "correct = sum(metrics_dataset[\"correct\"])\n",
    "guess = sum(metrics_dataset[\"guess\"])\n",
    "gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "precision = float(correct)/float(guess)\n",
    "recall = float(correct)/float(gold)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT_pred_dict_dataset = GT_pred_dataset.map(string_to_dict)\n",
    "# metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "# correct = sum(metrics_dataset[\"correct\"])\n",
    "# guess = sum(metrics_dataset[\"guess\"])\n",
    "# gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "# precision = float(correct)/float(guess)\n",
    "# recall = float(correct)/float(gold)\n",
    "# f1_score = 2*precision*recall/(precision+recall)\n",
    "# print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_test_dataset = test_dataset.map(entities_relation_GT)\n",
    "GT_pred_dataset = GT_test_dataset.map(get_prediction)\n",
    "GT_pred_dataset.save_to_disk(\"./datasets/bright-shape-68/GT_pred_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_relation_redfm(example):\n",
    "\n",
    "#   RELATION_NAMES=['country', 'place of birth', 'spouse', 'country of citizenship', 'instance of',\n",
    "#     'capital', 'child', 'shares border with', 'author', 'director', 'occupation',\n",
    "#       'founded by', 'league', 'owned by', 'genre', 'named after', 'follows',\n",
    "#       'headquarters location', 'cast member', 'manufacturer',\n",
    "#         'located in or next to body of water', 'location', 'part of', \n",
    "#         'mouth of the watercourse', 'member of', 'sport', 'characters',\n",
    "#           'participant', 'notable work', 'replaces', 'sibling', 'inception']\n",
    "#   relations = []\n",
    "#   for relation in example['relations']:\n",
    "#     relation_dict = {}\n",
    "#     relation_dict[\"object\"] = relation['object']['surfaceform']\n",
    "#     relation_dict[\"subject\"] = relation['subject']['surfaceform']\n",
    "#     relation_dict[\"predicate\"] = RELATION_NAMES[relation['predicate']]\n",
    "#     relations.append(relation_dict)\n",
    "#   return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### Instruction:\n",
    "You are an expert in data science and natural language processing (NLP).\n",
    "Your task is to extract ALL triplets from the text provided below.\n",
    "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
    "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
    "Multiple triplets must be in list form.\n",
    "Text: Le chat mange la souris.\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'object': 'Somme', 'subject': 'Mers-les-Bains', 'predicate': 'location'},\n",
       " {'object': 'années 1960', 'subject': 'années 1970', 'predicate': 'follows'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relation_redfm(prompt_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solar-serenity-74 run (used datacollator with instruction and response template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download the checkpoint and dataset if haven't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solar-serenity-74 run\n",
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/checkpoint-ql6d9loh:v10', type='model')\n",
    "# artifact_dir = artifact.download(root=\"./checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('xianli/digital_safety/SREDFM-dataset:v9', type='dataset')\n",
    "# artifact_dir = artifact.download(root=\"./datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. load the checkpoint and merge with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DEVICE INFERENCE IS RUNNING ON IS:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557e9614295b44ba8b195e7e9608f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artifact_dir = \"./checkpoints/solar-serenity-74/checkpoint-ql6d9loh:v10\"\n",
    "ft_model = Model(checkpoint_dir=artifact_dir)\n",
    "ft_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7eda98bb3d426a81dbe80a9fbc5dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:363: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f5bf649e341238b883d9e9af253cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"./datasets/solar-serenity-74/SREDFM-dataset:v9/test\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "GT_test_dataset = test_dataset.map(entities_relation_GT)\n",
    "GT_pred_dataset = GT_test_dataset.map(get_prediction)\n",
    "GT_pred_dataset.save_to_disk(\"./datasets/bright-shape-68/GT_pred_dataset\")\n",
    "# GT_pred_dict_dataset = GT_pred_dataset.map(string_to_dict)\n",
    "# metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "# correct = sum(metrics_dataset[\"correct\"])\n",
    "# guess = sum(metrics_dataset[\"guess\"])\n",
    "# gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "# precision = float(correct)/float(guess)\n",
    "# recall = float(correct)/float(gold)\n",
    "# f1_score = 2*precision*recall/(precision+recall)\n",
    "# print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an expert in data science and natural language processing (NLP).\n",
      "Your task is to extract triples from the text provided below.\n",
      "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
      "['entity1', 'entity2', 'entity3', ...]\n",
      "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
      "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
      "Multiple triplets must be in list form.\n",
      "\n",
      "Text: Khattiya Sawasdiphol (thaï : พลตรี ขัตติยะ สวัสดิผล), alias Seh Daeng (thaï : เสธ. แดง ; français : « Le commandant rouge »), né le et tué le , est un général de division de l'armée thaïlandaise, autrefois affecté au commandement des opérations de sécurité interne. \n",
      "\n",
      "### Response:\n",
      "Entities: [\"général de division\", \"armée thaïlandaise\", \"Khattiya Sawasdiphol\"]\n",
      "\n",
      "Relations: [{\"Object\": \"armée thaïlandaise\", \"Predicate\": \"military branch\", \"Subject\": \"Khattiya Sawasdiphol\"}]</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(GT_pred_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction:\n",
      "You are an expert in data science and natural language processing (NLP).\n",
      "Your task is to extract triples from the text provided below.\n",
      "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
      "['entity1', 'entity2', 'entity3', ...]\n",
      "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
      "{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\n",
      "Multiple triplets must be in list form.\n",
      "\n",
      "Text: Khattiya Sawasdiphol (thaï : พลตรี ขัตติยะ สวัสดิผล), alias Seh Daeng (thaï : เสธ. แดง ; français : « Le commandant rouge »), né le et tué le , est un général de division de l'armée thaïlandaise, autrefois affecté au commandement des opérations de sécurité interne. \n",
      "\n",
      "### Response:\n",
      "Entities: [\"thaïlandaise\", \"Khattiya Sawasdiphol\", \"thaï\"]\n",
      "\n",
      "Relations: [{\"Object\": \"thaïlandaise\", \"Predicate\": \"country of citizenship\", \"Subject\": \"Khattiya Sawasdiphol\"}, {\"Object\": \"thaïlandaise\", \"Predicate\": \"country\", \"Subject\": \"thaï\"}]\n",
      "\n",
      "### Response:\n",
      "Entities: [\"thaïlandaise\", \"Khattiya Sawasdiphol\", \"thaï\"]\n",
      "\n",
      "Relations: [{\"Object\": \"thaïlandaise\", \"Predicate\": \"country of citizenship\", \"Subject\": \"Khattiya Sawasdiphol\"}, {\"Object\": \"thaïlandaise\", \"Predicate\": \"country\", \"Subject\": \"thaï\"}]\n",
      "\n",
      "### Response:\n",
      "Entities: [\"thaïlandaise\", \"Khattiya Sawasdiphol\", \"thaï\"]\n",
      "\n",
      "Relations: [{\"Object\": \"thaïlandaise\", \"Predicate\": \"country of citizenship\", \"Subject\": \"Khattiya Sawasdiphol\"}, {\"Object\": \"thaïlandaise\", \"\n"
     ]
    }
   ],
   "source": [
    "print(GT_pred_dataset[0]['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bright-shape-68 : training with data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9ce1d2af9944c8a5dcd0923c49baa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edea24ab5b8c4b238aae9c310872a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9cdd58c23b48598778e4addb6b263b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caf064212c6402aa0f7db604a729559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24878358366828857 0.45440494590417313 0.32153110047846895\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./datasets/bright-shape-68/SREDFM-dataset:v5/test/\"\n",
    "test_dataset = load_from_disk(dataset_path)\n",
    "GT_test_dataset = test_dataset.map(get_ground_truth)\n",
    "GT_pred_dataset = GT_test_dataset.map(get_prediction)\n",
    "GT_pred_dataset.save_to_disk(\"./datasets/bright-shape-68/GT_pred_dataset\")\n",
    "GT_pred_dict_dataset = GT_pred_dataset.map(string_to_dict)\n",
    "metrics_dataset = GT_pred_dict_dataset.map(get_true_positive)\n",
    "\n",
    "correct = sum(metrics_dataset[\"correct\"])\n",
    "guess = sum(metrics_dataset[\"guess\"])\n",
    "gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "precision = float(correct)/float(guess)\n",
    "recall = float(correct)/float(gold)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics of pions-sound-65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3334038502221282 0.5485555168813088 0.4147368421052632\n"
     ]
    }
   ],
   "source": [
    "correct = sum(metrics_dataset[\"correct\"])\n",
    "guess = sum(metrics_dataset[\"guess\"])\n",
    "gold = sum(metrics_dataset[\"gold\"])\n",
    "\n",
    "precision = float(correct)/float(guess)\n",
    "recall = float(correct)/float(gold)\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
