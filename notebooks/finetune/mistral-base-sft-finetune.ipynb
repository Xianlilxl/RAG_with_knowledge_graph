{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You only need to run this once per machine\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 870448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 3883\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 2079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_fr = load_dataset(\"Babelscape/SREDFM\", language=\"fr\")\n",
    "dataset_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_fr[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002372177405552196"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2079/(2079+3883+870448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004863833962319618"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13236/(13236+2701389+6685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 2701389\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 6685\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 13236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from create_dataset import load_and_concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "dataset_en = load_dataset(\"Babelscape/SREDFM\", language=\"en\")\n",
    "dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_lengths, ec='black', bins=15)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Text Sequence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1752820\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from create_dataset import load_and_concatenate_datasets\n",
    "dataset_name = \"Babelscape/SREDFM\"\n",
    "dataset = load_and_concatenate_datasets(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ef52d9184b482f9bcfa74e0e791fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1752820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 498936\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 1024\n",
    "new_dataset = dataset.filter(lambda example : len(example[\"text\"])<MAX_LENGTH)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 493946\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2495\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2495\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from create_dataset import redo_train_test_split\n",
    "split_dataset = redo_train_test_split(new_dataset)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005000641364824346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2495/(2495*2+493946)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28646340734885944"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "498703/1740896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03695596188514035"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "287/7766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1226974\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 262923\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 262923\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "concat_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "# shuffle the dataset before the new split\n",
    "shuffled_dataset = concat_dataset.shuffle(seed=80)\n",
    "train_test_dataset = shuffled_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "# Split the 30% test + valid in half test, half valid\n",
    "test_valid = train_test_dataset['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_fr = load_dataset(\"Babelscape/SREDFM\", language=\"fr\")\n",
    "concat_dataset_fr = concatenate_datasets([dataset_fr[\"train\"], dataset_fr[\"test\"], dataset_fr[\"validation\"]])\n",
    "\n",
    "dataset_en = load_dataset(\"Babelscape/SREDFM\", language=\"en\")\n",
    "concat_dataset_en = concatenate_datasets([dataset_en[\"train\"], dataset_en[\"test\"], dataset_en[\"validation\"]])\n",
    "sliced_dataset_en = (concat_dataset_en\n",
    "                     .shuffle(seed=42)\n",
    "                    .select(range(concat_dataset_fr.num_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "    num_rows: 1752820\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_datasets([concat_dataset_fr, sliced_dataset_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1752820\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from create_dataset import load_and_concatenate_datasets\n",
    "dataset = load_and_concatenate_datasets(\"Babelscape/SREDFM\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_dataset = dataset.filter(lambda example : len(example[\"text\"])<MAX_LENGTH, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 498936\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe15e93af5534975970d460504023922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/349255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c819184d6c014e47bfdb58b062686d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/74841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c112e28746c049e393100a035d54e369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/74840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from create_dataset import redo_train_test_split\n",
    "split_dataset = redo_train_test_split(filterd_dataset)\n",
    "split_dataset_path = './datasets/SREDFM-dataset-'+str(MAX_LENGTH)\n",
    "split_dataset.save_to_disk(split_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 349255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 74841\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 74840\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "loaded_dataset = load_from_disk(split_dataset_path)\n",
    "loaded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# train_dataset = load_dataset(\"Babelscape/REDFM\", language='fr', split='train')\n",
    "# eval_dataset = load_dataset(\"Babelscape/REDFM\", language='fr', split='validation')\n",
    "# test_dataset = load_dataset(\"Babelscape/REDFM\", language='fr', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "dataset_fr = load_dataset(\"Babelscape/SREDFM\", language='fr')\n",
    "dataset_en = load_dataset(\"Babelscape/SREDFM\", language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 2701389\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 6685\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 13236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 870448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 3883\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 2079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset_en[\"train\"].features.type == dataset_fr[\"train\"].features.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice the english dataset to match the french dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset_en = DatasetDict()\n",
    "sliced_dataset_en['train'] = dataset_en[\"train\"].shuffle(seed=42).select(range(dataset_fr[\"train\"].num_rows))\n",
    "sliced_dataset_en['test'] = dataset_en[\"test\"].shuffle(seed=42).select(range(dataset_fr[\"test\"].num_rows))\n",
    "sliced_dataset_en['validation'] = dataset_en[\"validation\"].shuffle(seed=42).select(range(dataset_fr[\"validation\"].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fr_relation(example):\n",
    "    entities_ls = example[\"entities\"]\n",
    "    relations = []\n",
    "    for relation in example['relations']:\n",
    "        relation_dict = {}\n",
    "        object_index = relation['object']\n",
    "        relation_dict[\"Objet\"] = entities_ls[object_index][\"surfaceform\"]\n",
    "        relation_dict[\"Prédicat\"] = relation['predicate']\n",
    "        subject_index = relation['subject']\n",
    "        relation_dict[\"Subjet\"] = entities_ls[subject_index][\"surfaceform\"]\n",
    "        relations.append(relation_dict)\n",
    "\n",
    "    return str(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_relation(example):\n",
    "    entities_ls = example[\"entities\"]\n",
    "    relations = []\n",
    "    for relation in example['relations']:\n",
    "        relation_dict = {}\n",
    "        object_index = relation['object']\n",
    "        relation_dict[\"Object\"] = entities_ls[object_index][\"surfaceform\"]\n",
    "        relation_dict[\"Predicate\"] = relation['predicate']\n",
    "        subject_index = relation['subject']\n",
    "        relation_dict[\"Subject\"] = entities_ls[subject_index][\"surfaceform\"]\n",
    "        relations.append(relation_dict)\n",
    "\n",
    "    return str(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(example):\n",
    "    entities_ls = list(set([entity[\"surfaceform\"] for entity in example[\"entities\"]]))\n",
    "    return str(entities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def get_fr_prompt_template():\n",
    "    fr_prompt_template = \"\"\"\n",
    "Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
    "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
    "Les entité s'agit du sujet et de l'objet d'une phrase, la liste d'entités doit être sous forme:\n",
    "['entité1', 'entité2', 'entité3', ...]\n",
    "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
    "{{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }}\n",
    "Les triples multiples doivent être sous forme de liste.\\n\n",
    "### TEXTE:\n",
    "{text}{eos_token}\\n\n",
    "### ENTITES:\n",
    "{entities}{eos_token}\\n\n",
    "### RELATIONS:\n",
    "{relations}{eos_token}\\n\n",
    "\"\"\"\n",
    "    return PromptTemplate(template=fr_prompt_template, input_variables=['text', 'eos_token', 'entities', 'relations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_prompt_template():\n",
    "    en_prompt_template = \"\"\"You are an expert in data science and natural language processing (NLP).\n",
    "Your task is to extract triples from the TEXT provided below.\n",
    "Entities are the subject and object of a sentence, the list of entities must be in the form:\n",
    "['entity1', 'entity2', 'entity3', ...]\n",
    "A knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \n",
    "{{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }}\n",
    "Multiple triples must be in list form.\\n\n",
    "### TEXT:\n",
    "{text}{eos_token}\\n\n",
    "### ENTITIES:\n",
    "{entities}{eos_token}\\n\n",
    "### RELATIONS:\n",
    "{relations}{eos_token}\\n\n",
    "\"\"\"\n",
    "    return PromptTemplate(template=en_prompt_template, input_variables=['text', 'eos_token', 'entities', 'relations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_fr_prompt(example):\n",
    "    template = get_fr_prompt_template()\n",
    "    full_prompt = template.format(\n",
    "        text=example[\"text\"],\n",
    "        eos_token=tokenizer.eos_token,\n",
    "        entities=get_entities(example),\n",
    "        relations=get_fr_relation(example)\n",
    "    )\n",
    "    return {\"text\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_en_prompt(example):\n",
    "    template = get_en_prompt_template()\n",
    "    full_prompt = template.format(\n",
    "        text=example[\"text\"],\n",
    "        eos_token=tokenizer.eos_token,\n",
    "        entities=get_entities(example),\n",
    "        relations=get_en_relation(example)\n",
    "    )\n",
    "    return {\"text\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e359bdecf6a34d3c98182323c8f252b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/870448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d6e13c9132425eafeb9096bb43bf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f5576b23e74a6b82725c0966b64cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 870448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3883\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset_en = sliced_dataset_en.map(generate_base_en_prompt, num_proc=4, remove_columns=['docid', 'title', 'uri', 'entities', 'relations'])\n",
    "prompt_dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6430861d1c49918c4cab9a1e05ab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/870448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffae522047e421caa49c5660c085aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b3477ac354788adcbc4d9cfcd5a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 870448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3883\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset_fr = dataset_fr.map(generate_base_fr_prompt, num_proc=4, remove_columns=['docid', 'title', 'uri', 'entities', 'relations'])\n",
    "prompt_dataset_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1740896\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 7766\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4158\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset_train = concatenate_datasets([prompt_dataset_en[\"train\"], prompt_dataset_fr[\"train\"]])\n",
    "concat_dataset_test = concatenate_datasets([prompt_dataset_en[\"test\"], prompt_dataset_fr[\"test\"]])\n",
    "concat_dataset_validation = concatenate_datasets([prompt_dataset_en[\"validation\"], prompt_dataset_fr[\"validation\"]])\n",
    "dataset_final = DatasetDict({\n",
    "    \"train\": concat_dataset_train,\n",
    "    \"test\": concat_dataset_test,\n",
    "    \"validation\": concat_dataset_validation\n",
    "}).shuffle(seed=80)\n",
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nDernier train pour Washington est le douzième album de la série de bande dessinée \"La Jeunesse de Blueberry\" de François Corteggiani (scénario), Michel Blanc-Dumont (dessin) et Claudine Blanc-Dumont (couleurs). Publié pour la première fois en 2001, c\\'est la troisième du cycle des complots divers (quatre tomes).</s>\\n\\n### ENTITES:\\n[\\'Dernier train pour Washington\\', \\'Claudine Blanc-Dumont\\', \\'2001\\', \\'La Jeunesse de Blueberry\\', \\'Michel Blanc-Dumont\\', \\'François Corteggiani\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'François Corteggiani\\', \\'Prédicat\\': \\'screenwriter\\', \\'Subjet\\': \\'Dernier train pour Washington\\'}, {\\'Objet\\': \\'Michel Blanc-Dumont\\', \\'Prédicat\\': \\'illustrator\\', \\'Subjet\\': \\'Dernier train pour Washington\\'}]</s>\\n\\n'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nPiero Weiss (né le à Trieste et mort le à Baltimore) est un pianiste et musicologue italo-américain. </s>\\n\\n### ENTITES:\\n[\\'Trieste\\', \\'Baltimore\\', \\'Piero Weiss\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'Trieste\\', \\'Prédicat\\': \\'place of birth\\', \\'Subjet\\': \\'Piero Weiss\\'}, {\\'Objet\\': \\'Baltimore\\', \\'Prédicat\\': \\'place of death\\', \\'Subjet\\': \\'Piero Weiss\\'}]</s>\\n\\n'}\n",
      "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nBatribolbus abas est une espèce de coléoptères de la famille des Staphylinidae et de la sous-famille des Pselaphinae.</s>\\n\\n### ENTITES:\\n[\\'espèce\\', \\'Staphylinidae\\', \\'famille\\', \\'sous-famille\\', \\'coléoptères\\', \\'Batribolbus abas\\', \\'Pselaphinae\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'espèce\\', \\'Prédicat\\': \\'taxon rank\\', \\'Subjet\\': \\'Batribolbus abas\\'}, {\\'Objet\\': \\'famille\\', \\'Prédicat\\': \\'taxon rank\\', \\'Subjet\\': \\'Staphylinidae\\'}, {\\'Objet\\': \\'famille\\', \\'Prédicat\\': \\'part of\\', \\'Subjet\\': \\'sous-famille\\'}, {\\'Objet\\': \\'Staphylinidae\\', \\'Prédicat\\': \\'parent taxon\\', \\'Subjet\\': \\'Pselaphinae\\'}, {\\'Objet\\': \\'sous-famille\\', \\'Prédicat\\': \\'taxon rank\\', \\'Subjet\\': \\'Pselaphinae\\'}]</s>\\n\\n'}\n",
      "{'text': 'You are an expert in data science and natural language processing (NLP).\\nYour task is to extract triples from the TEXT provided below.\\nEntities are the subject and object of a sentence, the list of entities must be in the form:\\n[\\'entity1\\', \\'entity2\\', \\'entity3\\', ...]\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triples must be in list form.\\n\\n### TEXT:\\nTolombeh-ye Shahid Allah Verdi (, also Romanized as Tolombeh-ye Shahīd Allah Verdī) is a village in Golzar Rural District, in the Central District of Bardsir County, Kerman Province, Iran. At the 2006 census, its population was 102, in 21 families.</s>\\n\\n### ENTITIES:\\n[\\'21\\', \\'Tolombeh-ye Shahid Allah Verdi\\', \\'Iran\\', \\'2006\\', \\'Kerman Province\\', \\'Golzar Rural District\\', \\'Bardsir County\\', \\'Romanize\\', \\'Central District\\']</s>\\n\\n### RELATIONS:\\n[{\\'Object\\': \\'Golzar Rural District\\', \\'Predicate\\': \\'located in the administrative territorial entity\\', \\'Subject\\': \\'Tolombeh-ye Shahid Allah Verdi\\'}, {\\'Object\\': \\'Iran\\', \\'Predicate\\': \\'country\\', \\'Subject\\': \\'Tolombeh-ye Shahid Allah Verdi\\'}, {\\'Object\\': \\'Central District\\', \\'Predicate\\': \\'located in the administrative territorial entity\\', \\'Subject\\': \\'Golzar Rural District\\'}, {\\'Object\\': \\'Bardsir County\\', \\'Predicate\\': \\'part of\\', \\'Subject\\': \\'Golzar Rural District\\'}, {\\'Object\\': \\'Iran\\', \\'Predicate\\': \\'country\\', \\'Subject\\': \\'Golzar Rural District\\'}, {\\'Object\\': \\'Bardsir County\\', \\'Predicate\\': \\'located in the administrative territorial entity\\', \\'Subject\\': \\'Central District\\'}, {\\'Object\\': \\'Iran\\', \\'Predicate\\': \\'country\\', \\'Subject\\': \\'Central District\\'}, {\\'Object\\': \\'Kerman Province\\', \\'Predicate\\': \\'located in the administrative territorial entity\\', \\'Subject\\': \\'Bardsir County\\'}, {\\'Object\\': \\'Iran\\', \\'Predicate\\': \\'country\\', \\'Subject\\': \\'Bardsir County\\'}, {\\'Object\\': \\'Iran\\', \\'Predicate\\': \\'located in the administrative territorial entity\\', \\'Subject\\': \\'Kerman Province\\'}]</s>\\n\\n'}\n",
      "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nVinod Dham (né en 1950 à Pune) est un ingénieur et entrepreneur indien.\\n\\nIl est connu comme le pour sa contribution au développement du microprocesseur Pentium d\\'Intel.</s>\\n\\n### ENTITES:\\n[\\'indien\\', \\'Vinod Dham\\', \\'microprocesseur\\', \\'1950\\', \\'Intel\\', \\'Pune\\', \\'Pentium\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'Pune\\', \\'Prédicat\\': \\'place of birth\\', \\'Subjet\\': \\'Vinod Dham\\'}, {\\'Objet\\': \\'indien\\', \\'Prédicat\\': \\'country of citizenship\\', \\'Subjet\\': \\'Vinod Dham\\'}, {\\'Objet\\': \\'indien\\', \\'Prédicat\\': \\'country\\', \\'Subjet\\': \\'Pune\\'}, {\\'Objet\\': \\'Intel\\', \\'Prédicat\\': \\'owned by\\', \\'Subjet\\': \\'Pentium\\'}, {\\'Objet\\': \\'Intel\\', \\'Prédicat\\': \\'manufacturer\\', \\'Subjet\\': \\'microprocesseur\\'}]</s>\\n\\n'}\n",
      "{'text': 'You are an expert in data science and natural language processing (NLP).\\nYour task is to extract triples from the TEXT provided below.\\nEntities are the subject and object of a sentence, the list of entities must be in the form:\\n[\\'entity1\\', \\'entity2\\', \\'entity3\\', ...]\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triples must be in list form.\\n\\n### TEXT:\\nPseudoscilla verdensis is a species of sea snail, a marine gastropod mollusk in the family Pyramidellidae, the pyrams and their allies.</s>\\n\\n### ENTITIES:\\n[\\'sea snail\\', \\'family\\', \\'marine\\', \\'mollusk\\', \\'Pyramidellidae\\', \\'gastropod\\', \\'Pseudoscilla verdensis\\', \\'species\\']</s>\\n\\n### RELATIONS:\\n[{\\'Object\\': \\'species\\', \\'Predicate\\': \\'taxon rank\\', \\'Subject\\': \\'Pseudoscilla verdensis\\'}, {\\'Object\\': \\'mollusk\\', \\'Predicate\\': \\'parent taxon\\', \\'Subject\\': \\'gastropod\\'}, {\\'Object\\': \\'family\\', \\'Predicate\\': \\'taxon rank\\', \\'Subject\\': \\'Pyramidellidae\\'}]</s>\\n\\n'}\n",
      "{'text': 'You are an expert in data science and natural language processing (NLP).\\nYour task is to extract triples from the TEXT provided below.\\nEntities are the subject and object of a sentence, the list of entities must be in the form:\\n[\\'entity1\\', \\'entity2\\', \\'entity3\\', ...]\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triples must be in list form.\\n\\n### TEXT:\\nHelen Gorrill (Helen Gørrill) is a British artist, curator, feminist and art historian. She was awarded a Doctorate in contemporary British Painting in 2017, co-supervised by the Royal College of Art. Her PhD thesis \"The Gendered Economic and Symbolic Values in Contemporary British Painting\" was subsequently acquired by the prestigious publishers I.B. Tauris/Bloomsbury.\\n\\nHelen Gorrill\\'s book \"Women Can\\'t Paint: Gender, the Glass Ceiling and Values in Contemporary Art\" was published in 2020.\\n\\nDr Gorrill\\'s artwork is included in private and public collections around the world, including in the New York Brooklyn Museum\\'s Elizabeth A. Sackler Center for Feminist Art\\'s digital archive. Helen Gørrill’s artwork explores ideas about time, history and reality; using contemporary imagery that juxtaposes with the Old Masters she sets out to reappropriate. Her new artwork focuses on vandalising old masters and reviving art historical portraits through photo bombing and incorporating elements from contemporary sub-cultures, and often incorporates media such as lipstick, eyeliner and human hair. Within this context, the striking images hover between the 17th century and today’s climate of uncertainty. Helen Gørrill\\'s collaged and painted portraits have been commissioned from prestigious clients such as the new Bankside Hotel adj. Tate Modern in London.\\n\\nHelen Gorrill\\'s doctoral and postdoctoral research discovered the emergence of a new \"Androgynous Aesthetics\" in contemporary British painting since the 1990s and an \"Essentialist Aesthetics\" in contemporary European painting. She also writes for The Guardian on matters of equality and diversity in the arts, arguing that publicly funded museums are partly responsible for discrimination and the low visibility of contemporary female artists operating today. She also writes for academic presses, such as the International Journal of the Arts in Society, on matters pertaining to art and gender and co-edits the Collective and Collaborative Drawing Conversations series of books published by Cambridge Scholars.\\n\\nGorrill\\'s artwork has also been the subject of some controversy. In 2009 her degree show, featuring drawings inspired by religious pamphlets that featured dominant women and sexually submissive men, was censored. In 2018 Gorrill stated that \"women can\\'t paint! There\\'s no such thing as a great woman artist!\". Guardian writer Henry Porter wrote, \"The male figures have been censored but to protect whom? The spam I receive contains more indecency than Ms Gorrill\\'s work. And it is much less interesting because she makes a valid point.\"</s>\\n\\n### ENTITIES:\\n[\\'equality and diversity\\', \\'1990\\', \\'collaged\\', \\'commissioned\\', \\'Androgynous\\', \\'2018\\', \\'Elizabeth A. Sackler Center for Feminist Art\\', \\'feminist\\', \\'Royal College of Art\\', \\'Cambridge Scholars\\', \\'17\\', \\'sub-cultures\\', \\'I.B. Tauris\\', \\'Guardian\\', \\'human hair\\', \\'17th century\\', \\'Helen Gorrill\\', \\'art historian\\', \\'2009\\', \\'lipstick\\', \\'sexually submissive men\\', \\'2017\\', \\'portraits\\', \\'Tate Modern\\', \\'Doctorate\\', \\'Bloomsbury.\\', \\'media\\', \\'contemporary British Painting\\', \\'curator\\', \\'The Guardian\\', \\'British\\', \\'discrimination\\', \\'2020\\', \\'eyeliner\\', \\'Henry Porter\\', \\'Old Masters\\', \\'photo bombing\\', \\'Brooklyn Museum\\']</s>\\n\\n### RELATIONS:\\n[{\\'Object\\': \\'curator\\', \\'Predicate\\': \\'occupation\\', \\'Subject\\': \\'Helen Gorrill\\'}, {\\'Object\\': \\'Brooklyn Museum\\', \\'Predicate\\': \\'part of\\', \\'Subject\\': \\'Elizabeth A. Sackler Center for Feminist Art\\'}]</s>\\n\\n'}\n",
      "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nConceição do Mato Dentro est une municipalité brésilienne de l\\'État du Minas Gerais et la microrégion de Conceição do Mato Dentro.</s>\\n\\n### ENTITES:\\n[\\'Minas Gerais\\', \\'microrégion de Conceição do Mato Dentro\\', \\'Conceição do Mato Dentro\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'Minas Gerais\\', \\'Prédicat\\': \\'located in the administrative territorial entity\\', \\'Subjet\\': \\'Conceição do Mato Dentro\\'}, {\\'Objet\\': \\'Minas Gerais\\', \\'Prédicat\\': \\'located in the administrative territorial entity\\', \\'Subjet\\': \\'Conceição do Mato Dentro\\'}]</s>\\n\\n'}\n",
      "{'text': 'You are an expert in data science and natural language processing (NLP).\\nYour task is to extract triples from the TEXT provided below.\\nEntities are the subject and object of a sentence, the list of entities must be in the form:\\n[\\'entity1\\', \\'entity2\\', \\'entity3\\', ...]\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triples must be in list form.\\n\\n### TEXT:\\nOperation Muscatine was a security operation conducted during the Vietnam War by the US Army in Quảng Ngãi Province, South Vietnam from 18 December 1967 to 10 June 1968. During this operation on 16 March 1968 the 1st Battalion, 20th Infantry Regiment and the 4th Battalion, 3rd Infantry Regiment carried out the My Lai Massacre.</s>\\n\\n### ENTITIES:\\n[\\'My Lai Massacre\\', \\'Quảng Ngãi Province\\', \\'Vietnam War\\', \\'South Vietnam\\', \\'Operation Muscatine\\', \\'1\\', \\'3\\', \\'4\\', \\'16 March 1968\\', \\'3rd Infantry Regiment\\', \\'10 June 1968\\', \\'20\\', \\'18 December 1967\\', \\'20th Infantry Regiment\\']</s>\\n\\n### RELATIONS:\\n[{\\'Object\\': \\'18 December 1967\\', \\'Predicate\\': \\'point in time\\', \\'Subject\\': \\'Operation Muscatine\\'}, {\\'Object\\': \\'South Vietnam\\', \\'Predicate\\': \\'participant\\', \\'Subject\\': \\'Vietnam War\\'}, {\\'Object\\': \\'16 March 1968\\', \\'Predicate\\': \\'point in time\\', \\'Subject\\': \\'My Lai Massacre\\'}]</s>\\n\\n'}\n",
      "{'text': 'You are an expert in data science and natural language processing (NLP).\\nYour task is to extract triples from the TEXT provided below.\\nEntities are the subject and object of a sentence, the list of entities must be in the form:\\n[\\'entity1\\', \\'entity2\\', \\'entity3\\', ...]\\nA knowledge triplet is made up of 2 entities (subject and object) linked by a predicate: \\n{\"Object\": \"\", \"Predicate\": \"\", \"Subject\": \"\" }\\nMultiple triples must be in list form.\\n\\n### TEXT:\\nQinhua Circuit (Chinese: , , \"Qīnhuàdào\") was a military governor–level circuit of China during the Tang dynasty. Its capital was Tanzhou (modern Changsha).</s>\\n\\n### ENTITIES:\\n[\\'Tanzhou\\', \\'Chinese\\', \\'circuit\\', \\'Changsha\\', \\'China\\', \\'Qinhua Circuit\\', \\'Tang dynasty\\']</s>\\n\\n### RELATIONS:\\n[{\\'Object\\': \\'China\\', \\'Predicate\\': \\'country\\', \\'Subject\\': \\'Chinese\\'}, {\\'Object\\': \\'Chinese\\', \\'Predicate\\': \\'language used\\', \\'Subject\\': \\'China\\'}]</s>\\n\\n'}\n",
      "{'text': '\\nVous êtes un expert en data science et en traitement du langage naturel(NLP).\\nVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\\nLes entité s\\'agit du sujet et de l\\'objet d\\'une phrase, la liste d\\'entités doit être sous forme:\\n[\\'entité1\\', \\'entité2\\', \\'entité3\\', ...]\\nUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \\n{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }\\nLes triples multiples doivent être sous forme de liste.\\n\\n### TEXTE:\\nGuillaume Herincx, né en et mort en , est un théologien franciscain, actif dans les Pays-Bas méridionaux, nommé évêque d\\'Ypres en 1677.</s>\\n\\n### ENTITES:\\n[\\'Guillaume Herincx\\', \\'Ypres\\', \\'Pays-Bas méridionaux\\', \\'évêque\\', \\'1677\\', \\'franciscain\\']</s>\\n\\n### RELATIONS:\\n[{\\'Objet\\': \\'franciscain\\', \\'Prédicat\\': \\'religious order\\', \\'Subjet\\': \\'Guillaume Herincx\\'}]</s>\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "for d in dataset_final[\"train\"].select(range(10)):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 870448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 3883\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['docid', 'title', 'uri', 'text', 'entities', 'relations'],\n",
       "        num_rows: 2079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dream Island (en français ) est une île de l'Antarctique située dans l'archipel Palmer, dans le nord de la terre de Graham. Elle se trouve à environ un kilomètre au sud-est du Cap Monaco, au large de la côte sud-ouest de l'île Anvers. \\n\\nL'île est inspectée en 1956-57 par des scientifiques britanniques du British Naval Hydrographic Survey Unit et nommé par l'UK Antarctic Place-Names Committee.\\n\\nSituée dans la péninsule Antarctique, une région revendiquée par le Chili, l'Argentine et le Royaume-Uni, elle tombe sous le traité sur l'Antarctique et aucune des revendications n'est actuellement reconnue.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "# base_model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "# base_model_id = \"bofenghuang/vigostral-7b-chat\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    full_prompt =f\"\"\"Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
    "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
    "Les entité s'agit du sujet et de l'objet d'une phrase, la liste d'entités doit être sous forme:\n",
    "['entité1', 'entité2', 'entité3', ...]\n",
    "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
    "{{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\": \"\" }}\n",
    "Les triples multiples doivent être sous forme de liste.\\n\n",
    "### TEXTE:\n",
    "{example[\"text\"]}{tokenizer.eos_token}\\n\n",
    "### ENTITES:\n",
    "{get_entities(example)}{tokenizer.eos_token}\\n\n",
    "### RELATIONS:\n",
    "{get_relation(example)}{tokenizer.eos_token}\\n\n",
    "\"\"\"\n",
    "    return {\"text\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Les entité s'agit du sujet et de l'objet d'une phrase, la liste d'entités doit être sous forme:\n",
      "['entité1', 'entité2', 'entité3', ...]\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "{\"Objet\": \"\",\"Prédicat\": \"\", \"Sujet\":\"\" }\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Darwinneon crypticus, unique représentant du genre Darwinneon, est une espèce d'araignées aranéomorphes de la famille des Salticidae.</s>\n",
      "\n",
      "### ENTITES:\n",
      "['aranéomorphes', 'genre', 'espèce', 'Salticidae', 'famille', 'araignées', 'Darwinneon crypticus']</s>\n",
      "\n",
      "### RELATIONS:\n",
      "[{'Objet': 'espèce', 'Prédicat': 'taxon rank', 'Subjet': 'Darwinneon crypticus'}, {'Objet': 'famille', 'Prédicat': 'taxon rank', 'Subjet': 'Salticidae'}]</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = dataset_fr[\"train\"][0]\n",
    "print(generate_prompt(example)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'Q33999',\n",
       "  'surfaceform': 'acteur',\n",
       "  'type': 'Concept',\n",
       "  'start': 29,\n",
       "  'end': 35},\n",
       " {'uri': 'Q3282637',\n",
       "  'surfaceform': 'producteur',\n",
       "  'type': 'Concept',\n",
       "  'start': 39,\n",
       "  'end': 49},\n",
       " {'uri': 'Q30',\n",
       "  'surfaceform': 'américain',\n",
       "  'type': 'LOC',\n",
       "  'start': 50,\n",
       "  'end': 59},\n",
       " {'uri': 'Q36091',\n",
       "  'surfaceform': 'Minneapolis',\n",
       "  'type': 'LOC',\n",
       "  'start': 68,\n",
       "  'end': 79},\n",
       " {'uri': 'Q1527',\n",
       "  'surfaceform': 'Minnesota',\n",
       "  'type': 'LOC',\n",
       "  'start': 89,\n",
       "  'end': 98},\n",
       " {'uri': 'Q5398426',\n",
       "  'surfaceform': 'séries télévisées',\n",
       "  'type': 'Concept',\n",
       "  'start': 153,\n",
       "  'end': 170},\n",
       " {'uri': 'Q210257',\n",
       "  'surfaceform': 'MacGyver',\n",
       "  'type': 'MEDIA',\n",
       "  'start': 172,\n",
       "  'end': 180},\n",
       " {'uri': 'Q187462',\n",
       "  'surfaceform': 'Stargate SG-1',\n",
       "  'type': 'MEDIA',\n",
       "  'start': 186,\n",
       "  'end': 199},\n",
       " {'uri': 'Q203047',\n",
       "  'surfaceform': 'Richard Dean Anderson',\n",
       "  'type': 'PER',\n",
       "  'start': 0,\n",
       "  'end': 21},\n",
       " {'uri': '-1^^http://www.w3.org/2001/XMLSchema#decimal',\n",
       "  'surfaceform': '-1',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 197,\n",
       "  'end': 199}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84e69fa8e334285b9b55563840bf71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/870448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmap(generate_prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmap(generate_prompt)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(example):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     full_prompt \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mVous êtes un expert en data science et en traitement du langage naturel(NLP).\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mVotre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mUn triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m[\u001b[39m\u001b[39m'\u001b[39m\u001b[39msujet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprédicat\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mLes triples multiples doivent être séparés par \u001b[39m\u001b[39m'\u001b[39m\u001b[39m | \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m### TEXTE:\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m{\u001b[39;00mexample[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39meos_token\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m### ENTITES:\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m{\u001b[39;00mget_entities(example)\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39meos_token\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m### RELATIONS:\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m{\u001b[39;00mget_relation(example)\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39meos_token\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: full_prompt}\n",
      "\u001b[1;32m/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m relations \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m relation \u001b[39min\u001b[39;00m example[\u001b[39m'\u001b[39m\u001b[39mrelations\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m relation[\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msurfaceform\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     subject \u001b[39m=\u001b[39m relation[\u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msurfaceform\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     predicate \u001b[39m=\u001b[39m RELATION_NAMES[relation[\u001b[39m'\u001b[39m\u001b[39mpredicate\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].map(generate_prompt)\n",
    "val_dataset = dataset[\"validation\"].map(generate_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set Up LoRA\n",
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3752071168 || trainable%: 6.993743675173274\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# print_trainable_parameters(model)\n",
    "\n",
    "# # Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "# model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxianli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/fine-tuning/wandb/run-20231019_154243-zwsqron7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xianli/digital_safety/runs/zwsqron7' target=\"_blank\">fluent-river-40</a></strong> to <a href='https://wandb.ai/xianli/digital_safety' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xianli/digital_safety' target=\"_blank\">https://wandb.ai/xianli/digital_safety</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xianli/digital_safety/runs/zwsqron7' target=\"_blank\">https://wandb.ai/xianli/digital_safety/runs/zwsqron7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xianli/digital_safety/runs/zwsqron7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc1a6c699d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, wandb\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"digital_safety\"\n",
    "os.environ[\"WANDB_BASE_URL\"]=\"https://api.wandb.ai\"\n",
    "wandb.init(project=\"digital_safety\", entity=\"xianli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba78f110e3e341dcb00912072f28fb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "La Princesse et la Grenouille (\"The Princess and the Frog\"), est le long-métrage d'animation et le « Classique d'animation » des studios Disney. Sorti en 2009, il est très librement inspiré du livre \"The Frog Princess\" de , lui-même une variante du conte \"Le Roi Grenouille ou Henri de Fer\", recueilli par les frères Grimm mais qui ne doit pas être confondu avec le conte russe \"La Princesse-Grenouille\".</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Le Roi Grenouille ou Henri de Fer’, ’auteur’, ’frères Grimm’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Tupai est un atoll faisant partie des îles Sous-le-Vent dans l'archipel de la Société. Il fait partie de la commune de Bora-Bora, et de la commune associée Faanui.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Tupai’, ’instance de’, ’atoll’] | [’Tupai’, ’localisation’, ’archipel de la Société’] | [’Tupai’, ’localisation’, ’Bora-Bora’] | [’îles Sous-le-Vent’, ’localisation’, ’archipel de la Société’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "est un environnement de développement intégré (EDI), placé en \"open source\" par Sun en juin 2000 sous licence CDDL (Common Development and Distribution License) et GPLv2. En plus de Java, NetBeans permet la prise en charge native de divers langages tels le C, le C++, le JavaScript, le XML, le Groovy, le PHP et le HTML, ou d'autres (dont Python et Ruby) par l'ajout de \"greffons\". Il offre toutes les facilités d'un IDE moderne (éditeur avec coloration syntaxique, projets multi-langage, refactoring, éditeur graphique d'interfaces et de pages Web).</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’NetBeans’, ’instance de’, ’environnement de développement intégré’] | [’NetBeans’, ’constructeur’, ’Sun’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Capgemini est une entreprise de services du numérique française créée par Serge Kampf en 1967 à Grenoble, sous le nom de Sogeti. Il s’agit du cabinet de conseil qui a le plus gros chiffre d'affaires du pays et elle figure parmi les dix plus grosses du secteur mondialement. Basée à Paris, la société fait partie du CAC 40 à la Bourse de Paris.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Capgemini’, ’pays’, ’française’] | [’Capgemini’, ’fondée par’, ’Serge Kampf’] | [’Capgemini’, ’instance de’, ’cabinet de conseil’] | [’Capgemini’, ’localisation du siège social’, ’Paris’] | [’Capgemini’, ’partie de’, ’CAC 40’] | [’Serge Kampf’, ’lieu de naissance’, ’Grenoble’] | [’Grenoble’, ’pays’, ’française’] | [’Sogeti’, ’fondée par’, ’Serge Kampf’] | [’Bourse de Paris’, ’localisation du siège social’, ’Paris’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Le Paléoprotérozoïque ou Protérozoïque inférieur est la première ère du Protérozoïque. Elle s’étend de . Cette époque est marquée par l'augmentation du taux d’oxygène, produit par des cyanobactéries. C'est aussi l'époque de l'apparition des plus anciens eukaryotes actuellement connus (les \"Grypania\") et des plus anciens organismes multicellulaires, ceux du groupe fossile de Franceville.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Paléoprotérozoïque’, ’instance de’, ’Protérozoïque’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "L'autre caractéristique de Shanghai est sa place majeure dans l'économie mondiale. L'émergence de la ville comme centre financier de l'Asie-Pacifique a d'abord eu lieu dans les années 1920 et 1930, concomitamment au développement des concessions européennes. La ville servait alors de porte d'entrée à la Chine. Shanghai connut également à cette époque un formidable essor culturel qui a beaucoup contribué à son aura mythique et fantasmatique. Le quartier du Bund sur la rive gauche du Huangpu est le témoin de cet âge d'or. Mais pendant la guerre sino-japonaise et la guerre civile, la ville a été bombardée et gravement endommagée. Shanghai a été finalement libéré en . </s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’1930’, ’suit’, ’années 1920’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/2000 00:06 < 1:08:07, 0.49 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Ce roman décrit un monde alternatif dans lequel l'Allemagne nazie, l'Empire du Japon et l'Italie fasciste ont remporté la Seconde Guerre mondiale et fait l'état des lieux quinze ans après cette victoire. </s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Seconde Guerre mondiale’, ’participant’, ’Allemagne nazie’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Le EB/Streymur est un club de football féroïen basé à Streymnes.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’EB/Streymur’, ’sport’, ’football’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Après la cession de Saab en 2010, le retrait de Daewoo en 2005, de Chevrolet en 2014 et la vente de GM Europe (Opel/Vauxhall) en 2017, le groupe américain ne possède plus de présence significative sur le continent européen, préférant concentrer ses activités sur des marchés plus rentables.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Vauxhall’, ’appartenant à’, ’Opel’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Rocky 4 (\"Rocky IV\") est un film américain écrit et réalisé par Sylvester Stallone et sorti en 1985.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Rocky 4’, ’directeur’, ’Sylvester Stallone’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Le territoire de Bornéo est partagé entre trois États souverains : le Brunei et la Malaisie au nord, et l'Indonésie au sud. </s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Brunei’, ’localisation’, ’Bornéo’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Le Googleplex est le siège social de Google, situé au 1600 Amphitheatre Parkway à Mountain View, en Californie près de San José. Googleplex est un mot-valise formé à partir de \"Google\" et de \"complex\" et est une référence au grand nombre gogolplex valant 10gogol = 1010100. Le Googleplex est l'un des 23 sites nord-américains de la firme. Google possède aussi 23 sites en Europe, 14 en Asie et dans le Pacifique, 5 au Moyen-Orient et 3 en Amérique latine.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Googleplex’, ’localisation’, ’Mountain View’] | [’Google’, ’localisation du siège social’, ’Mountain View’] | [’Googleplex’, ’nommé d'après’, ’gogolplex’] | [’gogolplex’, ’nommé d'après’, ’gogol’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Microsoft Outlook (officiellement Microsoft Office Outlook) est un gestionnaire d'informations personnelles et un client de courrier électronique propriétaire édité par Microsoft. Ce logiciel informatique fait partie de la suite bureautique Microsoft Office.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Microsoft Outlook’, ’instance de’, ’client de courrier électronique’] | [’Microsoft Outlook’, ’constructeur’, ’Microsoft’] | [’Microsoft Outlook’, ’partie de’, ’Microsoft Office’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "La Bastille ou Bastille Saint-Antoine, anciennement fort et bastide Saint-Anthoine lez Paris, est une forteresse construite au , à l'emplacement du débouché de la rue Saint-Antoine sur l’actuelle place de la Bastille à Paris. Devenue une prison, considérée comme le symbole du despotisme monarchique, elle fut totalement détruite lors de la Révolution française, après l'événement déclencheur devenu une fête nationale en France, la prise de la Bastille du .</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Bastille’, ’localisation’, ’place de la Bastille’] | [’Bastille’, ’localisation’, ’Paris’] | [’rue Saint-Antoine’, ’partage la frontière avec’, ’place de la Bastille’] | [’rue Saint-Antoine’, ’localisation’, ’Paris’] | [’place de la Bastille’, ’nommé d'après’, ’Bastille’] | [’prise de la Bastille’, ’partie de’, ’Révolution française’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Constituée en plusieurs étapes, elle réunit à terme la Mauritanie, le Sénégal, le Soudan français (devenu le Mali), la Guinée, la Côte d'Ivoire, le Niger, la Haute-Volta (devenue le Burkina Faso), le Togo et le Dahomey (devenu le Bénin). Sa superficie atteignait kilomètres carrés, soit environ sept fois celle de la France. Son chef-lieu était Saint-Louis (Sénégal) jusqu'en 1902, puis Dakar (Sénégal).</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Mali’, ’remplacer’, ’Soudan français’] | [’Côte d'Ivoire’, ’partage la frontière avec’, ’Burkina Faso’] | [’Niger’, ’partage la frontière avec’, ’Burkina Faso’] | [’Niger’, ’partage la frontière avec’, ’Bénin’] | [’Burkina Faso’, ’partage la frontière avec’, ’Bénin’] | [’Togo’, ’partage la frontière avec’, ’Bénin’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Aerovias del Continente Americano, ou Avianca (code AITA : AV ; code OACI : AVA) est une compagnie aérienne colombienne. C'est la plus ancienne compagnie aérienne en activité sur le continent américain et, juste après KLM, la compagnie aérienne au monde la plus ancienne à avoir conservé son nom initial. Depuis 2010 l'aviation fait partie du holding AviancaTaca.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Avianca’, ’instance de’, ’compagnie aérienne’] | [’Avianca’, ’pays’, ’colombie’] | [’KLM’, ’instance de’, ’compagnie aérienne’] | [’Avianca’, ’appartenant à’, ’AviancaTaca’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Loki, aussi appelé Loptr ou Hveðrungr (et Loge dans la tétralogie de Wagner), est le dieu de la malice, de la discorde et des illusions dans la mythologie nordique. Il est le fils du géant Farbauti et de Laufey. Loki est le père de plusieurs monstres : le serpent Jörmungand, le loup Fenrir et la déesse du monde des morts Hel. Il est également la mère du cheval d'Odin à huit jambes Sleipnir. Malgré ses origines, il est accueilli dans le panthéon divin des Ases par Odin. </s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Loki’, ’partie de’, ’mythologie nordique’] | [’Farbauti’, ’enfant’, ’Loki’] | [’Laufey’, ’enfant’, ’Loki’] | [’Loki’, ’membre de’, ’géant’] | [’Jörmungand’, ’frère et sœur’, ’Hel’] | [’Fenrir’, ’frère et sœur’, ’Hel’] | [’Sleipnir’, ’appartenant à’, ’Odin’] | [’Odin’, ’membre de’, ’Ases’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "(1930) Lucifer est un astéroïde évoluant dans la ceinture principale, découvert le par l'astronome américaine Elizabeth Roemer depuis Flagstaff.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’(1930) Lucifer’, ’instance de’, ’astéroïde’] | [’Elizabeth Roemer’, ’occupation’, ’astronome’] | [’Flagstaff’, ’pays’, ’américaine’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Google Agenda (appelé \"\" en anglais) est une application Google qui permet de partager des événements et des agendas et de les publier sur internet ou sur un site Web. Son utilisation nécessite d'ouvrir un compte Google. Il est disponible sur internet, iOS et Android</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Google Agenda’, ’constructeur’, ’Google’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Everytime est une chanson de l'artiste chanteuse américaine pop Britney Spears, issue de son quatrième album, \"In the Zone\". Celle-ci est parue le , sous Jive Records en tant que troisième single de l'album. Après sa rupture avec le chanteur pop, Justin Timberlake, Spears se lie d'amitié avec une choriste de ce dernier, Annet Artani. Ensemble, elles commencent à écrire des chansons dans la maison de Britney Spears à Los Angeles, et se rendent ensuite en Allemagne, où \"Everytime\" a été écrit. Spears a composé la musique au piano, et a développé les paroles avec Artani. La chanson a été écrite selon Artani comme une réponse au single de Timberlake, \"Cry Me a River\" (2002) ainsi qu'à diverses interviews radio données par le chanteur. Cependant, Britney Spears n'a jamais confirmé ou démenti ces allégations.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Everytime’, ’instance de’, ’chanson’] | [’Everytime’, ’genre’, ’pop’] | [’Everytime’, ’partie de’, ’In the Zone’] | [’chanson’, ’constructeur’, ’chanteuse’] | [’pop’, ’pays’, ’américaine’] | [’Britney Spears’, ’occupation’, ’chanteuse’] | [’Britney Spears’, ’genre’, ’pop’] | [’In the Zone’, ’genre’, ’pop’] | [’Justin Timberlake’, ’genre’, ’pop’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      "Q*bert est un jeu vidéo d'action/réflexion édité sur borne d'arcade par Gottlieb en 1982. Conçu par Warren Davis et Jeff Lee, \"Q*bert\" est devenu un classique du jeu d'arcade et a été porté sur de nombreux supports familiaux.</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’Q*bert’, ’instance de’, ’jeu vidéo’] | [’Q*bert’, ’genre’, ’réflexion’] | [’action’, ’instance de’, ’jeu vidéo’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### RELATIONS:` in the following instance: <s> Vous êtes un expert en data science et en traitement du langage naturel(NLP).\n",
      "Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous.\n",
      "Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : \n",
      "['sujet', 'prédicat', 'objet'].\n",
      "Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "### TEXTE:\n",
      " </ref>, et peut être contre le coronavirus SARS-CoV-2 responsable de la pandémie de maladie à coronavirus de 2019-2020. Autorisé sur le marché en janvier 2014, le Favipiravir est autorisé au Japon pour lutter contre les pandémies grippales en tant que médicament essentiel. Fujifilm Toyama Chemical a autorisé Zhejiang HISUN Pharmaceutical Co., Ltd. à produire ce médicament en République populaire de Chine, et en 2019, après l’expiration du brevet en Chine continentale,  d’autres fabricants ont également commencé à produire ce médicament (médicament générique).</s> \n",
      "\n",
      "### RELATIONS:\n",
      "[’SARS-CoV-2’, ’instance de’, ’coronavirus’]</s> \n",
      "\n",
      " This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 22.19 GiB of which 49.50 MiB is free. Including non-PyTorch memory, this process has 22.10 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 443.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb Cellule 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m trainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[transformers\u001b[39m.\u001b[39mEarlyStoppingCallback(\u001b[39m3\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B52.57.169.192/home/ubuntu/fine-tuning/mistral-base-sft-finetune.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/trainer.py:1506\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1505\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1506\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1507\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1508\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1509\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1510\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1511\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/trainer.py:1801\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1800\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1801\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1803\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1804\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1805\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1806\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1807\u001b[0m ):\n\u001b[1;32m   1808\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/trainer.py:2650\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2649\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2650\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2652\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2653\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/trainer.py:2673\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2672\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2673\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m   2674\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/utils/operations.py:647\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/peft/peft_model.py:965\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    955\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    956\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    957\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    963\u001b[0m         )\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    966\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    967\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    968\u001b[0m         inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    969\u001b[0m         labels\u001b[39m=\u001b[39mlabels,\n\u001b[1;32m    970\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    971\u001b[0m         output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    972\u001b[0m         return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    973\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    976\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    977\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:106\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1041\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1043\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1044\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1045\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1046\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1047\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1048\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1049\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1050\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1051\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1052\u001b[0m )\n\u001b[1;32m   1054\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1055\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:929\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    923\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    924\u001b[0m         hidden_states,\n\u001b[1;32m    925\u001b[0m         attention_mask,\n\u001b[1;32m    926\u001b[0m         position_ids,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    930\u001b[0m         hidden_states,\n\u001b[1;32m    931\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    932\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    933\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    934\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    935\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    936\u001b[0m         padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    937\u001b[0m     )\n\u001b[1;32m    939\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    941\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:632\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    630\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    631\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    633\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    635\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:195\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:269\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_layer\u001b[39m.\u001b[39mforward(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_layer\u001b[39m.\u001b[39mforward(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[39m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39m# sure.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mmatmul_4bit(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mt(), bias\u001b[39m=\u001b[39mbias, quant_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mquant_state)\n\u001b[1;32m    250\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear(A, F\u001b[39m.\u001b[39mdequantize_4bit(B, state)\u001b[39m.\u001b[39mto(A\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 22.19 GiB of which 49.50 MiB is free. Including non-PyTorch memory, this process has 22.10 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 443.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "project = \"KG-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project + \"_sft_r_32_alpha_64\"\n",
    "output_dir = \"./models/\" + run_name\n",
    "# output_dir = \"s3://dec-ds-xli-demologist/\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_steps=2000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=25,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",          # Name of the W&B run (optional)\n",
    "        load_best_model_at_end=True\n",
    "    ),\n",
    "    data_collator=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=\"### RELATIONS:\"),\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    "    callbacks=[transformers.EarlyStoppingCallback(3)]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base Mistral model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9c7553d45147e39c3c968b01183ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"bofenghuang/vigostral-7b-chat\"\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"../fine-tuning/models/mistral-KG-finetune/checkpoint-150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_pretrained(\"/home/ubuntu/fine-tuning/models/merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte: Chang'e 4 (du , de \"Chang'e\", déesse de la Lune dans la mythologie chinoise) est une sonde spatiale lunaire chinoise dont le lancement a eu lieu le . L'engin est une réplique de la sonde lunaire Chang'e 3, lancée en 2013. C'est le engin spatial chinois lancé vers la Lune et le deuxième à s'y poser. Chang'e 4 comprend un atterrisseur et un rover. Les deux engins spatiaux emportent plusieurs instruments dont des caméras, un spectromètre infrarouge pour mesurer la composition du sol à proximité du rover et un radar détectant la structure superficielle du sous-sol ainsi qu'un spectromètre radio pour analyser les éruptions solaires. La mission primaire doit durer 90 jours. \n",
      "Relations: [’Chang'e 4’, ’nommé d'après’, ’Chang'e’] | [’Chang'e 4’, ’instance de’, ’sonde spatiale’] | [’Chang'e 4’, ’suit’, ’Chang'e 3’]\n",
      "\n",
      "Vous êtes un expert en data science et en traitement du langage naturel(NLP). Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous. Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : ['sujet', 'prédicat', 'objet']. Les triples multiples doivent être séparés par ' | '.\n",
      "\n",
      "\n",
      "### Texte :\n",
      "Chang'e 4 (du , de \"Chang'e\", déesse de la Lune dans la mythologie chinoise) est une sonde spatiale lunaire chinoise dont le lancement a eu lieu le . L'engin est une réplique de la sonde lunaire Chang'e 3, lancée en 2013. C'est le engin spatial chinois lancé vers la Lune et le deuxième à s'y poser. Chang'e 4 comprend un atterrisseur et un rover. Les deux engins spatiaux emportent plusieurs instruments dont des caméras, un spectromètre infrarouge pour mesurer la composition du sol à proximité du rover et un radar détectant la structure superficielle du sous-sol ainsi qu'un spectromètre radio pour analyser les éruptions solaires. La mission primaire doit durer 90 jours. \n",
      "\n",
      "### Relations :\n",
      "ion | instance de | sonde spatiale\n",
      "Chang'e 4 | instance de | sonde spatiale\n",
      "Chang'e 4 | suit | Chang'e 3\n",
      "\n",
      "### Modules :\n",
      "\n",
      "### Multi-relations :\n",
      "\n",
      "### Entités :\n",
      "[’Chang'e 4’, ’instance de’, ’sonde spatiale’]\n",
      "[’Chang'e 4’, ’suit’, ’Chang'e 3’]\n",
      "\n",
      "### Relations :\n",
      "[’Chang'e 4’, ’instance de’, ’sonde spatiale’]\n",
      "[’Chang'e 4’, ’suit’, ’Chang'e 3’]\n",
      "[’Chang'e 3’, ’instance de’, ’sonde spatiale’]\n",
      "\n",
      "### Multi-relations :\n",
      "[’Chang'e 4’, ’instance de’, ’sonde spatiale’, ’suit’, ’Chang'e 3’]\n",
      "\n",
      "### Entités :\n",
      "[’sonde spatiale’, ’instance de’, ’engin spatial’]\n",
      "[’Chang'e 3’, ’instance de’, ’sonde spatiale’]\n",
      "\n",
      "### Relations :\n",
      "[’sonde spatiale’, ’instance de’, ’engin spatial’]\n",
      "[’Chang'e 3’, ’instance de’, ’sonde spat\n"
     ]
    }
   ],
   "source": [
    "example = test_dataset[1]\n",
    "print(\"Texte: \" + example['text'])\n",
    "print(\"Relations: \" + get_relation(example) + \"\\n\")\n",
    "\n",
    "eval_prompt = f\"\"\"Vous êtes un expert en data science et en traitement du langage naturel(NLP). Votre tâche consiste à extraire les triplets du TEXTE fourni ci-dessous. Un triplet de connaissances est constitué de 2 entités (sujet et objet) liées par un prédicat : ['sujet', 'prédicat', 'objet']. Les triples multiples doivent être séparés par ' | '.\\n\n",
    "\n",
    "### Texte :\n",
    "{example['text']}\n",
    "\n",
    "### Relations :\n",
    "\"\"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte: Chang'e 4 (du , de \"Chang'e\", déesse de la Lune dans la mythologie chinoise) est une sonde spatiale lunaire chinoise dont le lancement a eu lieu le . L'engin est une réplique de la sonde lunaire Chang'e 3, lancée en 2013. C'est le engin spatial chinois lancé vers la Lune et le deuxième à s'y poser. Chang'e 4 comprend un atterrisseur et un rover. Les deux engins spatiaux emportent plusieurs instruments dont des caméras, un spectromètre infrarouge pour mesurer la composition du sol à proximité du rover et un radar détectant la structure superficielle du sous-sol ainsi qu'un spectromètre radio pour analyser les éruptions solaires. La mission primaire doit durer 90 jours. \n",
      "Relations: [’Chang'e 4’, ’nommé d'après’, ’Chang'e’] | [’Chang'e 4’, ’instance de’, ’sonde spatiale’] | [’Chang'e 4’, ’suit’, ’Chang'e 3’]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example = test_dataset[1]\n",
    "print(\"Texte: \" + example['text'])\n",
    "print(\"Relations: \" + get_relation(example) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_venv",
   "language": "python",
   "name": "finetune_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
